{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GloVe_TweetsClassification.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP1I9dOZoKmKXlYnlzYCMQz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prawizard/TweetsClassification_NLP/blob/main/GloVe_TweetsClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0emta3hYwORC",
        "outputId": "68196d66-7b8a-4417-f710-85bc01e3b2a6"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip - ‘glove.6B.zip’ saved [862182613/862182613]"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-09 17:01:38--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-04-09 17:01:38--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-04-09 17:01:38--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.08MB/s    in 2m 54s  \n",
            "\n",
            "2021-04-09 17:04:33 (4.72 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "--2021-04-09 17:04:33--  http://-/\n",
            "Resolving - (-)... failed: Name or service not known.\n",
            "wget: unable to resolve host address ‘-’\n",
            "--2021-04-09 17:04:33--  http://xn--glove-sv3b.6b.xn--zip-to0a/\n",
            "Resolving xn--glove-sv3b.6b.xn--zip-to0a (xn--glove-sv3b.6b.xn--zip-to0a)... failed: Name or service not known.\n",
            "wget: unable to resolve host address ‘xn--glove-sv3b.6b.xn--zip-to0a’\n",
            "--2021-04-09 17:04:33--  http://saved/\n",
            "Resolving saved (saved)... failed: Name or service not known.\n",
            "wget: unable to resolve host address ‘saved’\n",
            "http://[862182613/862182613]: Invalid IPv6 numeric address.\n",
            "FINISHED --2021-04-09 17:04:33--\n",
            "Total wall clock time: 2m 55s\n",
            "Downloaded: 1 files, 822M in 2m 54s (4.72 MB/s)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxyDavi6wm5g"
      },
      "source": [
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile('glove.6B.zip', 'r')\n",
        "zip_ref.extractall('data')\n",
        "zip_ref.close()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPljk-qqdwVO"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "from keras.initializers import glorot_uniform"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1op3BDOoM5x"
      },
      "source": [
        "def read_glove_vecs(glove_file):\n",
        "    with open(glove_file, 'r') as f:\n",
        "        words = set()\n",
        "        word_to_vec_map = {}\n",
        "        for line in f:\n",
        "            line = line.strip().split()\n",
        "            curr_word = line[0]\n",
        "            words.add(curr_word)\n",
        "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
        "        \n",
        "        i = 1\n",
        "        words_to_index = {}\n",
        "        index_to_words = {}\n",
        "        for w in sorted(words):\n",
        "            words_to_index[w] = i\n",
        "            index_to_words[i] = w\n",
        "            i = i + 1\n",
        "    return words_to_index, index_to_words, word_to_vec_map\n",
        "\n",
        "def softmax(x):\n",
        "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum()\n",
        "\n",
        "\n",
        "def read_csv(filename = 'data/emojify_data.csv'):\n",
        "    phrase = []\n",
        "    emoji = []\n",
        "\n",
        "    with open (filename) as csvDataFile:\n",
        "        csvReader = csv.reader(csvDataFile)\n",
        "\n",
        "        for row in csvReader:\n",
        "            phrase.append(row[0])\n",
        "            emoji.append(row[1])\n",
        "\n",
        "    X = np.asarray(phrase)\n",
        "    Y = np.asarray(emoji, dtype=int)\n",
        "\n",
        "    return X, Y\n",
        "\n",
        "def convert_to_one_hot(Y, C):\n",
        "    Y = np.eye(C)[Y.reshape(-1)]\n",
        "    return Y\n",
        "\n",
        "\n",
        "emoji_dictionary = {\"0\": \"\\u2764\\uFE0F\",    # :heart: prints a black instead of red heart depending on the font\n",
        "                    \"1\": \":baseball:\",\n",
        "                    \"2\": \":smile:\",\n",
        "                    \"3\": \":disappointed:\",\n",
        "                    \"4\": \":fork_and_knife:\"}\n",
        "\n",
        "def label_to_emoji(label):\n",
        "    \"\"\"\n",
        "    Converts a label (int or string) into the corresponding emoji code (string) ready to be printed\n",
        "    \"\"\"\n",
        "    return emoji.emojize(emoji_dictionary[str(label)], use_aliases=True)\n",
        "              \n",
        "    \n",
        "def print_predictions(X, pred):\n",
        "    print()\n",
        "    for i in range(X.shape[0]):\n",
        "        print(X[i], label_to_emoji(int(pred[i])))\n",
        "        \n",
        "        \n",
        "def plot_confusion_matrix(y_actu, y_pred, title='Confusion matrix', cmap=plt.cm.gray_r):\n",
        "    \n",
        "    df_confusion = pd.crosstab(y_actu, y_pred.reshape(y_pred.shape[0],), rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
        "    \n",
        "    df_conf_norm = df_confusion / df_confusion.sum(axis=1)\n",
        "    \n",
        "    plt.matshow(df_confusion, cmap=cmap) # imshow\n",
        "    #plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(df_confusion.columns))\n",
        "    plt.xticks(tick_marks, df_confusion.columns, rotation=45)\n",
        "    plt.yticks(tick_marks, df_confusion.index)\n",
        "    #plt.tight_layout()\n",
        "    plt.ylabel(df_confusion.index.name)\n",
        "    plt.xlabel(df_confusion.columns.name)\n",
        "    \n",
        "    \n",
        "def predict(X, Y, W, b, word_to_vec_map):\n",
        "    \"\"\"\n",
        "    Given X (sentences) and Y (emoji indices), predict emojis and compute the accuracy of your model over the given set.\n",
        "    \n",
        "    Arguments:\n",
        "    X -- input data containing sentences, numpy array of shape (m, None)\n",
        "    Y -- labels, containing index of the label emoji, numpy array of shape (m, 1)\n",
        "    \n",
        "    Returns:\n",
        "    pred -- numpy array of shape (m, 1) with your predictions\n",
        "    \"\"\"\n",
        "    m = X.shape[0]\n",
        "    pred = np.zeros((m, 1))\n",
        "    \n",
        "    for j in range(m):                       # Loop over training examples\n",
        "        \n",
        "        # Split jth test example (sentence) into list of lower case words\n",
        "        words = X[j].lower().split()\n",
        "        \n",
        "        # Average words' vectors\n",
        "        avg = np.zeros((50,))\n",
        "        for w in words:\n",
        "            avg += word_to_vec_map[w]\n",
        "        avg = avg/len(words)\n",
        "\n",
        "        # Forward propagation\n",
        "        Z = np.dot(W, avg) + b\n",
        "        A = softmax(Z)\n",
        "        pred[j] = np.argmax(A)\n",
        "        \n",
        "    print(\"Accuracy: \"  + str(np.mean((pred[:] == Y.reshape(Y.shape[0],1)[:]))))\n",
        "    \n",
        "    return pred"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OowsCCJcm8fo"
      },
      "source": [
        "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('data/glove.6B.50d.txt')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygqYf-3szEop"
      },
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib.request\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.metrics import f1_score\n",
        "import re\n",
        "import requests"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hgV5krKzIgU"
      },
      "source": [
        "TRAIN_TEXT_URL=\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/emoji/train_text.txt\"\n",
        "TRAIN_LABELS_URL=\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/emoji/train_labels.txt\"\n",
        "VAL_TEXT_URL=\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/emoji/val_text.txt\"\n",
        "VAL_LABELS_URL=\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/emoji/val_labels.txt\"\n",
        "TEST_TEXT_URL=\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/emoji/test_text.txt\"\n",
        "TEST_LABELS_URL=\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/emoji/test_labels.txt\""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXeY2cjvzL9E",
        "outputId": "ceadce4c-0da5-4eed-ba0c-b279864a0804"
      },
      "source": [
        "r = requests.get(TRAIN_TEXT_URL, allow_redirects=True)\n",
        "open('train_text.txt', 'wb').write(r.content)\n",
        "\n",
        "r = requests.get(TRAIN_LABELS_URL, allow_redirects=True)\n",
        "open('train_labels.txt', 'wb').write(r.content)\n",
        "\n",
        "r = requests.get(VAL_TEXT_URL, allow_redirects=True)\n",
        "open('val_text.txt', 'wb').write(r.content)\n",
        "\n",
        "r = requests.get(VAL_LABELS_URL, allow_redirects=True)\n",
        "open('val_labels.txt', 'wb').write(r.content)\n",
        "\n",
        "r = requests.get(TEST_TEXT_URL, allow_redirects=True)\n",
        "open('test_text.txt', 'wb').write(r.content)\n",
        "\n",
        "r = requests.get(TEST_LABELS_URL, allow_redirects=True)\n",
        "open('test_labels.txt', 'wb').write(r.content)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "114435"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S07rMp1SzOnm"
      },
      "source": [
        "stream=open(\"train_text.txt\")\n",
        "tweets=stream.readlines()\n",
        "stream.close()\n",
        "\n",
        "val_stream=open(\"val_text.txt\")\n",
        "val_tweets=val_stream.readlines()\n",
        "val_stream.close()\n",
        "\n",
        "test_stream=open(\"test_text.txt\")\n",
        "test_tweets=test_stream.readlines()\n",
        "test_stream.close()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CrhFuXwL-wMx",
        "outputId": "714561be-79d1-4c33-ecbe-7fab8d91f676"
      },
      "source": [
        "tweets[7]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'More #tinyepic things #tinyepicwestern, this one is crazy @user I may be one of your… \\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDxnjDzgzWj1"
      },
      "source": [
        "for i in range(len(tweets)):\n",
        "  if tweets[i].find('@user')!=-1:\n",
        "    # tweets[i] = tweets[i].replace('@user',\"\")\n",
        "    tweets[i]=re.sub('@user', '', tweets[i])\n",
        "\n",
        "for i in range(len(val_tweets)):\n",
        "  if val_tweets[i].find('@user')!=-1:\n",
        "    val_tweets[i]=re.sub('@user', '', val_tweets[i])\n",
        "\n",
        "for i in range(len(test_tweets)):\n",
        "  if test_tweets[i].find('@user')!=-1:\n",
        "    test_tweets[i]=re.sub('@user', '', test_tweets[i])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Znw9LM22z4Ez"
      },
      "source": [
        "stream=open(\"train_labels.txt\")\n",
        "tweetsLabels=stream.readlines()\n",
        "stream.close()\n",
        "\n",
        "val_stream=open(\"val_labels.txt\")\n",
        "val_tweetsLabels=val_stream.readlines()\n",
        "val_stream.close()\n",
        "\n",
        "test_stream=open(\"test_labels.txt\")\n",
        "test_tweetsLabels=test_stream.readlines()\n",
        "test_stream.close()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wakBMjtz7Q-"
      },
      "source": [
        "labels=[0]*len(tweetsLabels)\n",
        "for i in range(len(tweetsLabels)):\n",
        "  if tweetsLabels[i].find('\\n')!=-1:\n",
        "    # tweets[i] = tweets[i].replace('@user',\"\")\n",
        "    labels[i]=int(re.sub('\\n', '', tweetsLabels[i]))\n",
        "# labels    \n",
        "# Words like effing converted to VetsResistSquadron\n",
        "\n",
        "val_labels=[0]*len(val_tweetsLabels)\n",
        "for i in range(len(val_tweetsLabels)):\n",
        "  if val_tweetsLabels[i].find('\\n')!=-1:\n",
        "    # tweets[i] = tweets[i].replace('@user',\"\")\n",
        "    val_labels[i]=int(re.sub('\\n', '', val_tweetsLabels[i]))\n",
        "\n",
        "test_labels=[0]*len(test_tweetsLabels)\n",
        "for i in range(len(test_tweetsLabels)):\n",
        "  if test_tweetsLabels[i].find('\\n')!=-1:\n",
        "    # tweets[i] = tweets[i].replace('@user',\"\")\n",
        "    test_labels[i]=int(re.sub('\\n', '', test_tweetsLabels[i]))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glB6CL4Vz99T"
      },
      "source": [
        "rows=[]\n",
        "rowIndices=[]\n",
        "for i in range(len(tweets)):\n",
        "  rows.append({\"TWEET\":tweets[i], \"CATEGORY\":labels[i]})\n",
        "  rowIndices.append(i+1)\n",
        "df=pd.DataFrame(rows, index=rowIndices)\n",
        "\n",
        "val_rows=[]\n",
        "val_rowIndices=[]\n",
        "for i in range(len(val_tweets)):\n",
        "  val_rows.append({\"TWEET\":val_tweets[i], \"CATEGORY\":val_labels[i]})\n",
        "  val_rowIndices.append(i+1)\n",
        "val_df=pd.DataFrame(val_rows, index=val_rowIndices)\n",
        "\n",
        "test_rows=[]\n",
        "test_rowIndices=[]\n",
        "for i in range(len(test_tweets)):\n",
        "  test_rows.append({\"TWEET\":test_tweets[i], \"CATEGORY\":test_labels[i]})\n",
        "  test_rowIndices.append(i+1)\n",
        "test_df=pd.DataFrame(test_rows, index=test_rowIndices)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNA-MinR0A6y",
        "outputId": "9660493f-c42f-4a59-deb8-a12625293d41"
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGhB6ANU0HJ5"
      },
      "source": [
        "\n",
        "# def clean_tweets(tweet, stemmer=PorterStemmer(), stop_words=set(stopwords.words('english'))):\n",
        "#   words=word_tokenize(tweet.lower())\n",
        "#   filtered_words=[]\n",
        "#   # for word in words:\n",
        "#   #   if word not in stop_words and word.isalpha():\n",
        "#   #     stemmed_word=stemmer.stem(word)\n",
        "#   #     filtered_words.append(stemmed_word)\n",
        "#   for word in words:\n",
        "#     if word.isalpha():\n",
        "#       # stemmed_word=stemmer.stem(word)\n",
        "#       filtered_words.append(word)\n",
        "#   return filtered_words"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYgvZQXR0KCJ"
      },
      "source": [
        "tweetsList=df.TWEET\n",
        "# nestedList=tweetsList.apply(clean_tweets)\n",
        "# nestedList[0:5]\n",
        "val_tweetsList=val_df.TWEET\n",
        "# val_nestedList=val_tweetsList.apply(clean_tweets)\n",
        "# val_nestedList[0:5]\n",
        "test_tweetsList=test_df.TWEET\n",
        "# test_nestedList=test_tweetsList.apply(clean_tweets)\n",
        "# test_nestedList[0:5]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzLI1Zx1HVD6",
        "outputId": "fd15d297-3124-4fc7-b3db-e685a18fea5b"
      },
      "source": [
        "np.asarray(tweetsList)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Sunday afternoon walking through Venice in the sun with  ️ ️ ️ @ Abbot Kinney, Venice \\n',\n",
              "       \"Time for some BBQ and whiskey libations. Chomp, belch, chomp! (@ Lucille's Smokehouse Bar-B-Que) \\n\",\n",
              "       'Love love love all these people ️ ️ ️ #friends #bff #celebrate #blessed #sundayfunday @ San… \\n',\n",
              "       ..., 'Be safe this weekend everyone. #happylaborday @ Beer NV \\n',\n",
              "       'Pizza (@ Five50 -  in Las Vegas, NV) \\n',\n",
              "       'my mini is perfect, no one deserves her @ Las Vegas, Nevada \\n'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yb5qBol20NQ0"
      },
      "source": [
        "# nestedList=tuple(nestedList)\n",
        "# val_nestedList=tuple(val_nestedList)\n",
        "# test_nestedList=tuple(test_nestedList)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCA2ToeT0TpJ"
      },
      "source": [
        "# uniquewords_train=set()\n",
        "# for tw in nestedList:\n",
        "#   for w in tw:\n",
        "#     uniquewords_train.add(w)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eK4IBL-I0XPF"
      },
      "source": [
        "# uniquewords_train=tuple(uniquewords_train)\n",
        "# word_id={}\n",
        "# id_word={}\n",
        "# i=1\n",
        "# for word in uniquewords_train:\n",
        "#   word_id[word]=i\n",
        "#   i+=1\n",
        "\n",
        "# for word, id in word_id.items():\n",
        "#   id_word[id]=word"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjaNi2cn0aTR"
      },
      "source": [
        "# X_train=[]\n",
        "# twt=[]\n",
        "# for tw in nestedList:\n",
        "#   for word in tw:\n",
        "#     wordid=word_id[word]\n",
        "#     twt.append(wordid)\n",
        "#   X_train.append(twt)\n",
        "#   twt=[]\n",
        "\n",
        "# twt=[]\n",
        "# X_val=[]\n",
        "# for tw in val_nestedList:\n",
        "#   for word in tw:\n",
        "#     wordid=word_id.get(word, 0)\n",
        "#     twt.append(wordid)\n",
        "#   X_val.append(twt)\n",
        "#   twt=[]\n",
        "\n",
        "# twt=[]\n",
        "# X_test=[]\n",
        "# for tw in test_nestedList:\n",
        "#   for word in tw:\n",
        "#     wordid=word_id.get(word, 0)\n",
        "#     twt.append(wordid)\n",
        "#   X_test.append(twt)\n",
        "#   twt=[]"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ou-oFBNp0dXy"
      },
      "source": [
        "y_train=df.CATEGORY\n",
        "y_val=val_df.CATEGORY\n",
        "y_test=test_df.CATEGORY"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQRsiacynEkg"
      },
      "source": [
        "# X_train, Y_train = read_csv('data/train_emoji.csv')\n",
        "# X_test, Y_test = read_csv('data/tesss.csv')\n",
        "X_train=np.array(df.TWEET)\n",
        "X_test=np.asarray(test_df.TWEET)\n",
        "Y_train=np.asarray(y_train)\n",
        "Y_test=np.asarray(y_test)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGAmJUBHqQVr"
      },
      "source": [
        "X_train=X_train.squeeze()\n",
        "X_test=X_test.squeeze()\n"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TX7XvDK2m9hj",
        "outputId": "553e9785-4ef8-4da1-a99c-2e397cc731a5"
      },
      "source": [
        "maxLen = len(max(X_train, key=len).split())\n",
        "maxLen"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WzuL7KQqnuh",
        "outputId": "5c9360ea-c772-48b2-baa0-e953fc780160"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(45000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gW4Ttxb5ggKn"
      },
      "source": [
        "def sentences_to_indices(X, word_to_index, max_len):\n",
        "    \n",
        "    m = X.shape[0]                                   # number of training examples\n",
        "    \n",
        "    X_indices = np.zeros((m, max_len))\n",
        "    \n",
        "    for i in range(m):                               # loop over training examples\n",
        "        \n",
        "        sentence_words =X[i].lower().split()\n",
        "        \n",
        "        j = 0\n",
        "        \n",
        "        for w in sentence_words:\n",
        "          if j<max_len:\n",
        "            X_indices[i, j] = word_to_index.get(w, 0)\n",
        "            j = j + 1\n",
        "            \n",
        "    \n",
        "    return X_indices"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTgSfzp4hE1w"
      },
      "source": [
        "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
        "    \n",
        "    vocab_len = len(word_to_index) + 1                  # adding 1 to fit Keras embedding (requirement)\n",
        "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]      # define dimensionality of your GloVe word vectors (= 50)\n",
        "    \n",
        "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
        "    \n",
        "    for word, index in word_to_index.items():\n",
        "        emb_matrix[index, :] = word_to_vec_map[word]\n",
        "\n",
        "    embedding_layer = Embedding(vocab_len, emb_dim, trainable=False)\n",
        "    \n",
        "    embedding_layer.build((None,)) # Do not modify the \"None\".  This line of code is complete as-is.\n",
        "    \n",
        "    embedding_layer.set_weights([emb_matrix])\n",
        "    \n",
        "    return embedding_layer"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_IcZjcghWl-"
      },
      "source": [
        "def Emojify_V2(input_shape, word_to_vec_map, word_to_index):\n",
        "    \n",
        "    sentence_indices = Input(input_shape, dtype='int32')\n",
        "    \n",
        "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
        "    \n",
        "    embeddings = embedding_layer(sentence_indices)    \n",
        "    \n",
        "    X = LSTM(128, return_sequences=True)(embeddings)\n",
        "    \n",
        "    X = Dropout(0.5)(X)\n",
        "    \n",
        "    X = LSTM(128, return_sequences=False)(X)\n",
        "    \n",
        "    X = Dropout(0.5)(X)\n",
        "    \n",
        "    X = Dense(20)(X)\n",
        "    \n",
        "    X = Activation('softmax')(X)\n",
        "    \n",
        "    model = Model(inputs=sentence_indices, outputs=X)\n",
        "      \n",
        "    return model"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDv8xhX7h9Mw",
        "outputId": "8b0db8dd-6bc1-4949-da89-5599d2b3084f"
      },
      "source": [
        "model = Emojify_V2((maxLen,), word_to_vec_map, word_to_index)\n",
        "model.summary()"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_11\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_12 (InputLayer)        [(None, 29)]              0         \n",
            "_________________________________________________________________\n",
            "embedding_11 (Embedding)     (None, 29, 50)            20000050  \n",
            "_________________________________________________________________\n",
            "lstm_22 (LSTM)               (None, 29, 128)           91648     \n",
            "_________________________________________________________________\n",
            "dropout_22 (Dropout)         (None, 29, 128)           0         \n",
            "_________________________________________________________________\n",
            "lstm_23 (LSTM)               (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "dropout_23 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 20)                2580      \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 20)                0         \n",
            "=================================================================\n",
            "Total params: 20,225,862\n",
            "Trainable params: 225,812\n",
            "Non-trainable params: 20,000,050\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCz2ZHwwiJBf"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-ibTZzqnVTh"
      },
      "source": [
        "X_train_indices = sentences_to_indices(X_train, word_to_index, maxLen)\n",
        "Y_train_oh = convert_to_one_hot(Y_train, C = 20)"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ccD1Pt4nXKh",
        "outputId": "acc3887c-c32a-4555-c3ee-6cd5c3dce178"
      },
      "source": [
        "model.fit(X_train_indices, Y_train_oh, epochs = 150, batch_size = 32, shuffle=True)"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 1.2821 - accuracy: 0.6054\n",
            "Epoch 2/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 1.2754 - accuracy: 0.6074\n",
            "Epoch 3/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 1.2587 - accuracy: 0.6127\n",
            "Epoch 4/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 1.2433 - accuracy: 0.6148\n",
            "Epoch 5/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 1.2404 - accuracy: 0.6166\n",
            "Epoch 6/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 1.2272 - accuracy: 0.6217\n",
            "Epoch 7/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 1.2199 - accuracy: 0.6238\n",
            "Epoch 8/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 1.2091 - accuracy: 0.6272\n",
            "Epoch 9/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 1.1963 - accuracy: 0.6270\n",
            "Epoch 10/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 1.1888 - accuracy: 0.6316\n",
            "Epoch 11/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 1.1818 - accuracy: 0.6346\n",
            "Epoch 12/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 1.1663 - accuracy: 0.6384\n",
            "Epoch 13/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 1.1632 - accuracy: 0.6400\n",
            "Epoch 14/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 1.1537 - accuracy: 0.6418\n",
            "Epoch 15/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 1.1396 - accuracy: 0.6456\n",
            "Epoch 16/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 1.1382 - accuracy: 0.6462\n",
            "Epoch 17/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 1.1234 - accuracy: 0.6484\n",
            "Epoch 18/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 1.1269 - accuracy: 0.6519\n",
            "Epoch 19/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 1.1055 - accuracy: 0.6581\n",
            "Epoch 20/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 1.1024 - accuracy: 0.6566\n",
            "Epoch 21/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 1.0995 - accuracy: 0.6584\n",
            "Epoch 22/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 1.0920 - accuracy: 0.6620\n",
            "Epoch 23/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 1.0920 - accuracy: 0.6617\n",
            "Epoch 24/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 1.0749 - accuracy: 0.6650\n",
            "Epoch 25/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 1.0696 - accuracy: 0.6673\n",
            "Epoch 26/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 1.0693 - accuracy: 0.6682\n",
            "Epoch 27/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 1.0565 - accuracy: 0.6719\n",
            "Epoch 28/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 1.0525 - accuracy: 0.6730\n",
            "Epoch 29/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 1.0442 - accuracy: 0.6744\n",
            "Epoch 30/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 1.0343 - accuracy: 0.6789\n",
            "Epoch 31/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 1.0339 - accuracy: 0.6768\n",
            "Epoch 32/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 1.0168 - accuracy: 0.6830\n",
            "Epoch 33/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 1.0175 - accuracy: 0.6810\n",
            "Epoch 34/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 1.0130 - accuracy: 0.6843\n",
            "Epoch 35/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 1.0110 - accuracy: 0.6857\n",
            "Epoch 36/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 1.0076 - accuracy: 0.6881\n",
            "Epoch 37/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.9890 - accuracy: 0.6922\n",
            "Epoch 38/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.9912 - accuracy: 0.6910\n",
            "Epoch 39/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.9956 - accuracy: 0.6898\n",
            "Epoch 40/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.9829 - accuracy: 0.6931\n",
            "Epoch 41/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.9771 - accuracy: 0.6946\n",
            "Epoch 42/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.9695 - accuracy: 0.6994\n",
            "Epoch 43/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.9662 - accuracy: 0.6979\n",
            "Epoch 44/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.9518 - accuracy: 0.7025\n",
            "Epoch 45/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.9454 - accuracy: 0.7064\n",
            "Epoch 46/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.9465 - accuracy: 0.7072\n",
            "Epoch 47/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.9419 - accuracy: 0.7069\n",
            "Epoch 48/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.9464 - accuracy: 0.7046\n",
            "Epoch 49/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.9423 - accuracy: 0.7074\n",
            "Epoch 50/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.9327 - accuracy: 0.7094\n",
            "Epoch 51/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.9212 - accuracy: 0.7137\n",
            "Epoch 52/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.9261 - accuracy: 0.7129\n",
            "Epoch 53/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.9222 - accuracy: 0.7121\n",
            "Epoch 54/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.9211 - accuracy: 0.7138\n",
            "Epoch 55/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.9018 - accuracy: 0.7199\n",
            "Epoch 56/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.9123 - accuracy: 0.7138\n",
            "Epoch 57/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.9011 - accuracy: 0.7199\n",
            "Epoch 58/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.8922 - accuracy: 0.7206\n",
            "Epoch 59/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.8869 - accuracy: 0.7209\n",
            "Epoch 60/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.8899 - accuracy: 0.7236\n",
            "Epoch 61/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.8895 - accuracy: 0.7225\n",
            "Epoch 62/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.8795 - accuracy: 0.7247\n",
            "Epoch 63/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.8832 - accuracy: 0.7226\n",
            "Epoch 64/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.8728 - accuracy: 0.7280\n",
            "Epoch 65/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.8665 - accuracy: 0.7294\n",
            "Epoch 66/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.8713 - accuracy: 0.7279\n",
            "Epoch 67/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.8740 - accuracy: 0.7266\n",
            "Epoch 68/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.8649 - accuracy: 0.7335\n",
            "Epoch 69/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.8540 - accuracy: 0.7327\n",
            "Epoch 70/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.8476 - accuracy: 0.7378\n",
            "Epoch 71/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.8439 - accuracy: 0.7359\n",
            "Epoch 72/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.8526 - accuracy: 0.7364\n",
            "Epoch 73/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.8419 - accuracy: 0.7372\n",
            "Epoch 74/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.8429 - accuracy: 0.7383\n",
            "Epoch 75/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.8466 - accuracy: 0.7384\n",
            "Epoch 76/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.8416 - accuracy: 0.7397\n",
            "Epoch 77/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.8299 - accuracy: 0.7401\n",
            "Epoch 78/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.8152 - accuracy: 0.7456\n",
            "Epoch 79/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.8294 - accuracy: 0.7429\n",
            "Epoch 80/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.8181 - accuracy: 0.7448\n",
            "Epoch 81/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.8202 - accuracy: 0.7475\n",
            "Epoch 82/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.8129 - accuracy: 0.7483\n",
            "Epoch 83/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.8137 - accuracy: 0.7467\n",
            "Epoch 84/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.8095 - accuracy: 0.7480\n",
            "Epoch 85/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.8028 - accuracy: 0.7502\n",
            "Epoch 86/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.8024 - accuracy: 0.7496\n",
            "Epoch 87/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.8112 - accuracy: 0.7488\n",
            "Epoch 88/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.8009 - accuracy: 0.7513\n",
            "Epoch 89/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.7785 - accuracy: 0.7560\n",
            "Epoch 90/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.7943 - accuracy: 0.7544\n",
            "Epoch 91/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.7974 - accuracy: 0.7507\n",
            "Epoch 92/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.7902 - accuracy: 0.7541\n",
            "Epoch 93/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.7773 - accuracy: 0.7597\n",
            "Epoch 94/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.7767 - accuracy: 0.7609\n",
            "Epoch 95/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.7794 - accuracy: 0.7580\n",
            "Epoch 96/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.7823 - accuracy: 0.7541\n",
            "Epoch 97/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.7657 - accuracy: 0.7614\n",
            "Epoch 98/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.7725 - accuracy: 0.7610\n",
            "Epoch 99/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.7639 - accuracy: 0.7623\n",
            "Epoch 100/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.7686 - accuracy: 0.7587\n",
            "Epoch 101/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.7681 - accuracy: 0.7616\n",
            "Epoch 102/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.7610 - accuracy: 0.7625\n",
            "Epoch 103/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.7640 - accuracy: 0.7624\n",
            "Epoch 104/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.7619 - accuracy: 0.7622\n",
            "Epoch 105/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.7516 - accuracy: 0.7654\n",
            "Epoch 106/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.7538 - accuracy: 0.7635\n",
            "Epoch 107/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.7530 - accuracy: 0.7687\n",
            "Epoch 108/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.7535 - accuracy: 0.7674\n",
            "Epoch 109/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.7475 - accuracy: 0.7684\n",
            "Epoch 110/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.7422 - accuracy: 0.7714\n",
            "Epoch 111/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.7455 - accuracy: 0.7699\n",
            "Epoch 112/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.7376 - accuracy: 0.7686\n",
            "Epoch 113/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.7424 - accuracy: 0.7706\n",
            "Epoch 114/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.7376 - accuracy: 0.7731\n",
            "Epoch 115/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.7316 - accuracy: 0.7722\n",
            "Epoch 116/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.7340 - accuracy: 0.7735\n",
            "Epoch 117/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.7261 - accuracy: 0.7762\n",
            "Epoch 118/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.7448 - accuracy: 0.7716\n",
            "Epoch 119/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.7287 - accuracy: 0.7764\n",
            "Epoch 120/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.7137 - accuracy: 0.7796\n",
            "Epoch 121/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.7135 - accuracy: 0.7790\n",
            "Epoch 122/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.7181 - accuracy: 0.7764\n",
            "Epoch 123/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.7091 - accuracy: 0.7813\n",
            "Epoch 124/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.7200 - accuracy: 0.7774\n",
            "Epoch 125/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.7245 - accuracy: 0.7770\n",
            "Epoch 126/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.7141 - accuracy: 0.7796\n",
            "Epoch 127/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.7141 - accuracy: 0.7791\n",
            "Epoch 128/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.7063 - accuracy: 0.7814\n",
            "Epoch 129/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.6989 - accuracy: 0.7823\n",
            "Epoch 130/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.7097 - accuracy: 0.7812\n",
            "Epoch 131/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.7054 - accuracy: 0.7845\n",
            "Epoch 132/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.6992 - accuracy: 0.7838\n",
            "Epoch 133/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.6999 - accuracy: 0.7854\n",
            "Epoch 134/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.7006 - accuracy: 0.7826\n",
            "Epoch 135/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.6854 - accuracy: 0.7890\n",
            "Epoch 136/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.6888 - accuracy: 0.7862\n",
            "Epoch 137/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.6908 - accuracy: 0.7874\n",
            "Epoch 138/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.6860 - accuracy: 0.7890\n",
            "Epoch 139/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.6883 - accuracy: 0.7886\n",
            "Epoch 140/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.6842 - accuracy: 0.7890\n",
            "Epoch 141/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.6790 - accuracy: 0.7899\n",
            "Epoch 142/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.6953 - accuracy: 0.7866\n",
            "Epoch 143/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.6758 - accuracy: 0.7928\n",
            "Epoch 144/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.6724 - accuracy: 0.7924\n",
            "Epoch 145/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.6782 - accuracy: 0.7926\n",
            "Epoch 146/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.6857 - accuracy: 0.7905\n",
            "Epoch 147/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.6749 - accuracy: 0.7931\n",
            "Epoch 148/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.6687 - accuracy: 0.7938\n",
            "Epoch 149/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.6726 - accuracy: 0.7934\n",
            "Epoch 150/150\n",
            "1407/1407 [==============================] - 11s 8ms/step - loss: 0.6763 - accuracy: 0.7931\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fdd93be1f90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTjQI6SVnY_7",
        "outputId": "9ec92139-04b7-4b7d-d04a-f4a1c241f48a"
      },
      "source": [
        "X_test_indices = sentences_to_indices(X_test, word_to_index, max_len = maxLen)\n",
        "Y_test_oh = convert_to_one_hot(Y_test, C = 20)\n",
        "loss, acc = model.evaluate(X_test_indices, Y_test_oh)\n",
        "print()\n",
        "print(\"Test accuracy = \", acc)"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1563/1563 [==============================] - 7s 4ms/step - loss: 5.1711 - accuracy: 0.2149\n",
            "\n",
            "Test accuracy =  0.21493999660015106\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJoujdng1QnF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}