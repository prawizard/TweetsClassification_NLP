{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment_Analysis_NLP_Progress.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prawizard/TweetsClassification_NLP/blob/main/TweetEval/Sentiment_Analysis_NLP_Progress.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zxm9gIFFi02"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pD7oNRf_BCiU"
      },
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import urllib.request\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.metrics import f1_score\n",
        "import requests\n",
        "import collections"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgN_9rGQyena"
      },
      "source": [
        "\n",
        "import os, string, collections\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import snowballstemmer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn import metrics\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers import Flatten, Dense, Dropout, Convolution1D, MaxPooling1D, SpatialDropout1D, Input \n",
        "from keras.layers import GlobalMaxPooling1D, concatenate, LSTM, Bidirectional\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDKThEBPVrEE"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,Flatten,Embedding,Activation, Dropout\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D \n",
        "import numpy as np\n",
        "from numpy import array\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GT7sQy4ZFttE"
      },
      "source": [
        "# Constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlB9kkYFFotD"
      },
      "source": [
        "TRAIN_TEXT_URL=\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/offensive/train_text.txt\"\n",
        "TRAIN_LABELS_URL=\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/offensive/train_labels.txt\"\n",
        "VAL_TEXT_URL=\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/offensive/val_text.txt\"\n",
        "VAL_LABELS_URL=\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/offensive/val_labels.txt\"\n",
        "TEST_TEXT_URL=\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/offensive/test_text.txt\"\n",
        "TEST_LABELS_URL=\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/offensive/test_labels.txt\"\n",
        "VOCAB_SIZE=2000"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igvM4blPFz8b"
      },
      "source": [
        "# Access Data from the Files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhTRYuieF-LC"
      },
      "source": [
        "## Download the .txt files to our runtime"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eP1mSZRFpBF",
        "outputId": "89156889-1112-4338-b1ea-c5e079db0f42"
      },
      "source": [
        "r = requests.get(TRAIN_TEXT_URL, allow_redirects=True)\n",
        "open('train_text.txt', 'wb').write(r.content)\n",
        "\n",
        "r = requests.get(TRAIN_LABELS_URL, allow_redirects=True)\n",
        "open('train_labels.txt', 'wb').write(r.content)\n",
        "\n",
        "r = requests.get(VAL_TEXT_URL, allow_redirects=True)\n",
        "open('val_text.txt', 'wb').write(r.content)\n",
        "\n",
        "r = requests.get(VAL_LABELS_URL, allow_redirects=True)\n",
        "open('val_labels.txt', 'wb').write(r.content)\n",
        "\n",
        "r = requests.get(TEST_TEXT_URL, allow_redirects=True)\n",
        "open('test_text.txt', 'wb').write(r.content)\n",
        "\n",
        "r = requests.get(TEST_LABELS_URL, allow_redirects=True)\n",
        "open('test_labels.txt', 'wb').write(r.content)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1720"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgJlnjLsGPHE"
      },
      "source": [
        "## Read data from the files downloaded"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewS1DCGvFpHb"
      },
      "source": [
        "# Tweets\n",
        "stream=open(\"train_text.txt\")\n",
        "tweets=stream.readlines()\n",
        "stream.close()\n",
        "val_stream=open(\"val_text.txt\")\n",
        "val_tweets=val_stream.readlines()\n",
        "val_stream.close()\n",
        "test_stream=open(\"test_text.txt\")\n",
        "test_tweets=test_stream.readlines()\n",
        "test_stream.close()\n",
        "\n",
        "# Labels\n",
        "stream=open(\"train_labels.txt\")\n",
        "tweetsLabels=stream.readlines()\n",
        "stream.close()\n",
        "val_stream=open(\"val_labels.txt\")\n",
        "val_tweetsLabels=val_stream.readlines()\n",
        "val_stream.close()\n",
        "test_stream=open(\"test_labels.txt\")\n",
        "test_tweetsLabels=test_stream.readlines()\n",
        "test_stream.close()\n",
        "\n",
        "\n",
        "# Labels\n",
        "labels=[0]*len(tweetsLabels)\n",
        "for i in range(len(tweetsLabels)):\n",
        "  if tweetsLabels[i].find('\\n')!=-1:\n",
        "    labels[i]=int(re.sub('\\n', '', tweetsLabels[i]))\n",
        "val_labels=[0]*len(val_tweetsLabels)\n",
        "for i in range(len(val_tweetsLabels)):\n",
        "  if val_tweetsLabels[i].find('\\n')!=-1:\n",
        "    val_labels[i]=int(re.sub('\\n', '', val_tweetsLabels[i]))\n",
        "test_labels=[0]*len(test_tweetsLabels)\n",
        "for i in range(len(test_tweetsLabels)):\n",
        "  if test_tweetsLabels[i].find('\\n')!=-1:\n",
        "    test_labels[i]=int(re.sub('\\n', '', test_tweetsLabels[i]))\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jh8aZrlQFpNM",
        "outputId": "5908e731-fb01-4b21-80fc-1d387f90578a"
      },
      "source": [
        "print('Samples in Training set : ',len(labels),', Validation set : ', len(val_labels),', Test set : ', len(test_labels))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Samples in Training set :  11916 , Validation set :  1324 , Test set :  860\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTbHMhp-JxUk"
      },
      "source": [
        "# Pre-processing of the text in tweets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7z2JpSWKK3b"
      },
      "source": [
        "## Remove the '@User' text parts from tweets\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ND7rNLwzJ5gm"
      },
      "source": [
        "\n",
        "for i in range(len(tweets)):\n",
        "  if tweets[i].find('@user')!=-1:\n",
        "    tweets[i]=re.sub('@user', '', tweets[i])\n",
        "\n",
        "for i in range(len(val_tweets)):\n",
        "  if val_tweets[i].find('@user')!=-1:\n",
        "    val_tweets[i]=re.sub('@user', '', val_tweets[i])\n",
        "\n",
        "for i in range(len(test_tweets)):\n",
        "  if test_tweets[i].find('@user')!=-1:\n",
        "    test_tweets[i]=re.sub('@user', '', test_tweets[i])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgqCpZNSKW_0"
      },
      "source": [
        "## Remove hashtags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hXCHYWMJ5d7"
      },
      "source": [
        "for i in range(len(tweets)):\n",
        "  if tweets[i].find('#[a-zA-Z]+')!=-1:\n",
        "    tweets[i]=re.sub('#[a-zA-Z]+', '', tweets[i])\n",
        "\n",
        "for i in range(len(val_tweets)):\n",
        "  if val_tweets[i].find('#[a-zA-Z]+')!=-1:\n",
        "    val_tweets[i]=re.sub('#[a-zA-Z]+', '', val_tweets[i])\n",
        "\n",
        "for i in range(len(test_tweets)):\n",
        "  if test_tweets[i].find('#[a-zA-Z]+')!=-1:\n",
        "    test_tweets[i]=re.sub('#[a-zA-Z]+', '', test_tweets[i])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3lvlpSpLW11"
      },
      "source": [
        "## Remove escape characters, unnecessary characters if present and correct wrongly spelt words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pY-TDJ8WJ5ar"
      },
      "source": [
        "for i in range(len(tweets)):\n",
        "  if tweets[i].find('\\n')!=-1:\n",
        "    tweets[i]=re.sub('\\n', '', tweets[i])\n",
        "\n",
        "for i in range(len(val_tweets)):\n",
        "  if val_tweets[i].find('\\n')!=-1:\n",
        "    val_tweets[i]=re.sub('\\n', '', val_tweets[i])\n",
        "\n",
        "for i in range(len(test_tweets)):\n",
        "  if test_tweets[i].find('\\n')!=-1:\n",
        "    test_tweets[i]=re.sub('\\n', '', test_tweets[i])\n",
        "\n",
        "# Unnecessary dots\n",
        "p='\\.\\.\\.|\\.\\.'\n",
        "\n",
        "for i in range(len(tweets)):\n",
        "  tweets[i]=re.sub(p, '', tweets[i])\n",
        "  tweets[i]=tweets[i].lower()\n",
        "\n",
        "for i in range(len(val_tweets)):\n",
        "  val_tweets[i]=re.sub(p, '', val_tweets[i])\n",
        "  val_tweets[i]=val_tweets[i].lower()\n",
        "\n",
        "for i in range(len(test_tweets)):\n",
        "  test_tweets[i]=re.sub(p, '', test_tweets[i])\n",
        "  test_tweets[i]=test_tweets[i].lower()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPK6DFYJai3p"
      },
      "source": [
        "rows=[]\n",
        "rowIndices=[]\n",
        "for i in range(len(tweets)):\n",
        "  rows.append({\"TWEET\":tweets[i], \"CATEGORY\":labels[i]})\n",
        "  rowIndices.append(i+1)\n",
        "df=pd.DataFrame(rows, index=rowIndices)\n",
        "\n",
        "val_rows=[]\n",
        "val_rowIndices=[]\n",
        "for i in range(len(val_tweets)):\n",
        "  val_rows.append({\"TWEET\":val_tweets[i], \"CATEGORY\":val_labels[i]})\n",
        "  val_rowIndices.append(i+1)\n",
        "val_df=pd.DataFrame(val_rows, index=val_rowIndices)\n",
        "\n",
        "test_rows=[]\n",
        "test_rowIndices=[]\n",
        "for i in range(len(test_tweets)):\n",
        "  test_rows.append({\"TWEET\":test_tweets[i], \"CATEGORY\":test_labels[i]})\n",
        "  test_rowIndices.append(i+1)\n",
        "test_df=pd.DataFrame(test_rows, index=test_rowIndices)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MstYIClUev7S"
      },
      "source": [
        "frames=[df, val_df, test_df]\n",
        "df=pd.concat(frames)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nC7x0RmBaiyJ",
        "outputId": "f166c23c-9690-4804-8ba9-d0546590e274"
      },
      "source": [
        "type(df)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.frame.DataFrame"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "cXse8yQ8aiva",
        "outputId": "770d778a-8990-4a06-e94b-4fa769ff0d1d"
      },
      "source": [
        "df"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TWEET</th>\n",
              "      <th>CATEGORY</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>bono who cares. soon people will understand t...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>eight years the republicans denied obama’s pi...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>get him some line help. he is gonna be just f...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>she is great. hi fiona!</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>she has become a parody unto herself? she has...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>856</th>\n",
              "      <td>#cnn irrationally argues 4 legalising #abortio...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>857</th>\n",
              "      <td>city of chicago, democrat run wi...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>858</th>\n",
              "      <td>#conservatives don’t care what you postit’s  p...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>859</th>\n",
              "      <td>#antifa #resist trump is trying to bring world...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>860</th>\n",
              "      <td>#maine you need to face facts  doesn’t really ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14100 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 TWEET  CATEGORY\n",
              "1     bono who cares. soon people will understand t...         0\n",
              "2     eight years the republicans denied obama’s pi...         1\n",
              "3     get him some line help. he is gonna be just f...         0\n",
              "4                             she is great. hi fiona!          0\n",
              "5     she has become a parody unto herself? she has...         1\n",
              "..                                                 ...       ...\n",
              "856  #cnn irrationally argues 4 legalising #abortio...         0\n",
              "857                city of chicago, democrat run wi...         0\n",
              "858  #conservatives don’t care what you postit’s  p...         1\n",
              "859  #antifa #resist trump is trying to bring world...         0\n",
              "860  #maine you need to face facts  doesn’t really ...         0\n",
              "\n",
              "[14100 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bwk0QJELaisS",
        "outputId": "12831835-d54d-4792-f12f-3fb5990ba8d5"
      },
      "source": [
        "df['CATEGORY'].value_counts()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    9460\n",
              "1    4640\n",
              "Name: CATEGORY, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaFjjNEdaikK",
        "outputId": "5457a124-cdbf-4ae8-c7ad-bbd8108dd050"
      },
      "source": [
        "text = df['TWEET'].tolist()\n",
        "text[:10]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' bono who cares. soon people will understand that they gain nothing from following a phony celebrity. become a leader of your people instead or help and support your fellow countrymen. ',\n",
              " ' eight years the republicans denied obama’s picks. breitbarters outrage is as phony as their fake president. ',\n",
              " ' get him some line help. he is gonna be just fine. as the game went on you could see him progressing more with his reads. he brought what has been missing. the deep ball presence. now he just needs a little more time ',\n",
              " '  she is great. hi fiona! ',\n",
              " \" she has become a parody unto herself? she has certainly taken some heat for being such an.well idiot. could be optic too  who know with liberals  they're all optics.  no substance \",\n",
              " '               this is the vetsresistsquadron\"\" is bullshit they are girl scout veterans, i have never met any other veterans or served with anyone that was a gun control advocate? have you?\"\" ',\n",
              " ' your looking more like a plant #maga #walkaway ',\n",
              " '  lol. except he’s the most successful president in our lifetimes. he’s undone most of the damage obummer did and set america on the right path again. #maga ',\n",
              " ' been a willie fan since before most of you were born.love that he is holding a rally with beto. exactly which fans are furious?  could you give some specifics? ',\n",
              " \" here's a link to my channel with a plethora of topics to peruse: \"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9EYIcwqdLd-"
      },
      "source": [
        "y = df['CATEGORY']"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZvX83DbdLjq",
        "outputId": "3026f390-86fa-4356-9234-bc0aa28492f5"
      },
      "source": [
        "token = Tokenizer()\n",
        "token.fit_on_texts(text)\n",
        "token"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras_preprocessing.text.Tokenizer at 0x7f2e45514f90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oeT9J6pfdLqK",
        "outputId": "dea02d21-b54c-49c9-fde9-042b95f7d6e0"
      },
      "source": [
        "vocab_size = len(token.word_index) + 1\n",
        "vocab_size"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23433"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nek6_FDzdLwT",
        "outputId": "3e33f367-87d0-49db-b69c-4170b9176cd0"
      },
      "source": [
        "import itertools \n",
        "print(dict(itertools.islice(token.index_word.items(), 100)))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{1: 'the', 2: 'is', 3: 'to', 4: 'a', 5: 'and', 6: 'you', 7: 'of', 8: 'are', 9: 'i', 10: 'he', 11: 'in', 12: 'that', 13: 'for', 14: 'she', 15: 'it', 16: 'this', 17: 'on', 18: 'not', 19: 'with', 20: 'they', 21: 'have', 22: 'be', 23: 'liberals', 24: 'gun', 25: 'so', 26: 'all', 27: 'control', 28: 'antifa', 29: 'your', 30: 'like', 31: 'what', 32: 'as', 33: 'but', 34: 'we', 35: 'just', 36: 'about', 37: 'her', 38: 'maga', 39: 'was', 40: 'if', 41: 'conservatives', 42: 'will', 43: 'do', 44: 'who', 45: 'people', 46: 'no', 47: 'his', 48: 'at', 49: 'or', 50: 'my', 51: 'by', 52: 'has', 53: 'from', 54: 'how', 55: 'an', 56: 'out', 57: 'up', 58: 'their', 59: 'me', 60: 'get', 61: 'one', 62: 'amp', 63: 'know', 64: 'trump', 65: 'why', 66: 'when', 67: 'more', 68: 'because', 69: 'can', 70: 'him', 71: 'now', 72: 'them', 73: 'think', 74: \"don't\", 75: 'there', 76: 'would', 77: 'should', 78: 'right', 79: 'our', 80: 'good', 81: 'only', 82: 'us', 83: \"it's\", 84: 'want', 85: 'time', 86: 'go', 87: 'see', 88: 'going', 89: 'need', 90: 'never', 91: 'than', 92: 'shit', 93: 'been', 94: 'being', 95: 'don’t', 96: 'then', 97: 'love', 98: 'even', 99: 'these', 100: 'say'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_7w-lgcdL1t",
        "outputId": "f0204b6f-8ccd-4003-ff44-23362a79a099"
      },
      "source": [
        "x = ['i to the a and']\n",
        "token.texts_to_sequences(x)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[9, 3, 1, 4, 5]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxGkvfuldL7G",
        "outputId": "369f3536-3b92-4181-eadb-fb5697de6534"
      },
      "source": [
        "encoded_text = token.texts_to_sequences(text)\n",
        "print(encoded_text[:30])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1370, 44, 694, 532, 45, 42, 324, 12, 20, 1923, 145, 53, 542, 4, 2279, 3978, 437, 4, 660, 7, 29, 45, 480, 49, 182, 5, 159, 29, 1638, 10016], [3979, 147, 1, 263, 4601, 2875, 3980, 10017, 1566, 2, 32, 2279, 32, 58, 285, 135], [60, 70, 107, 543, 182, 10, 2, 415, 22, 35, 517, 32, 1, 298, 544, 17, 6, 154, 87, 70, 10018, 67, 19, 47, 10019, 10, 1125, 31, 52, 93, 1371, 1, 569, 2021, 3501, 71, 10, 35, 224, 4, 287, 67, 85], [14, 2, 138, 1208, 3981], [14, 52, 437, 4, 6990, 10020, 746, 14, 52, 1372, 850, 107, 5520, 13, 94, 188, 55, 117, 570, 154, 22, 10021, 102, 44, 63, 19, 23, 361, 26, 5521, 46, 6991], [16, 2, 1, 10022, 2, 533, 20, 8, 362, 10023, 2022, 9, 21, 90, 1080, 101, 118, 2022, 49, 2639, 19, 213, 12, 39, 4, 24, 27, 1924, 21, 6], [29, 288, 67, 30, 4, 2876, 38, 352], [161, 586, 260, 1, 119, 1253, 135, 11, 79, 10024, 260, 6992, 119, 7, 1, 2133, 10025, 105, 5, 639, 174, 17, 1, 78, 2431, 158, 38], [93, 4, 3127, 649, 220, 204, 119, 7, 6, 110, 1126, 97, 12, 10, 2, 1081, 4, 1209, 19, 951, 368, 185, 980, 8, 1567, 154, 6, 192, 107, 6993], [2280, 4, 1568, 3, 50, 2877, 19, 4, 10026, 7, 3502, 3, 10027], [28, 76, 1425, 4, 41, 397, 128, 5, 599, 76, 22, 75, 10028, 1, 10029, 62, 1639, 2640, 17, 1, 5522], [20, 6994, 1827, 94, 1210, 13, 3982, 10030, 6995, 75, 8, 827, 10031, 2878, 1, 155, 40, 18, 628, 342, 7, 92, 40, 6, 8, 88, 3, 390, 1, 213, 69, 60, 1210, 1496, 1640, 99, 45, 123, 2641, 5523, 5, 5524, 10032], [545, 4602, 554, 2642, 7, 3503, 5, 96, 300, 3, 3128, 112, 24, 27, 450, 197, 1569], [10033], [2643, 77, 21, 93, 1321, 112, 4, 245, 348, 301, 30, 1, 1735, 42, 222, 70, 6996, 1, 3983, 78, 148, 1, 6997, 221, 117], [16, 2, 4, 447, 1082, 53, 23, 5, 113, 294, 3129, 2432, 34, 21, 3, 2879, 1, 650, 184, 4603, 253, 184, 5, 96, 3504, 87, 922, 11, 15, 10034], [123, 10035, 2644, 119, 7, 1, 10036, 1211, 124, 194, 506, 12, 17, 59], [117, 87, 40, 9, 273, 264, 48, 2281, 36, 902, 24, 27, 167, 5, 14, 3984, 59, 49, 775, 59, 4, 3505, 2433, 10037, 2880, 32, 14, 105, 96, 34, 600, 322, 4, 1641, 127, 35, 264, 148, 1, 5525], [6, 241, 1, 103, 523, 1083, 10038, 26, 7, 2023, 2881], [138, 2882, 75, 599, 243, 18, 25, 114, 38, 10039], [6, 8, 25, 10040, 15, 239, 22, 281, 3, 373, 12, 103], [26, 6, 43, 2, 357, 26, 534, 122, 82, 357, 38], [10041, 3985], [10, 123, 10042, 12, 75, 2, 1, 1164, 2134, 66, 34, 8, 2434, 1126, 96, 75, 2, 1, 6998, 2134, 185, 2, 728, 66, 34, 10043, 828, 3, 79, 4604, 6999, 5, 22, 10044, 10045, 160, 94, 7000, 51, 47, 2024, 48, 7001, 66, 79, 4605, 1642, 2, 470, 3506, 187], [65, 8, 6, 2282, 85, 7002, 17, 259, 3986, 17, 2883, 7, 45, 289, 335, 12, 1497, 387, 30, 208, 2, 35, 4, 1007, 10046, 1322, 10047, 3, 118, 5526, 12, 20, 172, 2645, 51, 64], [34, 115, 10048, 2646, 587, 1, 3130, 747, 33, 34, 69, 2647, 729, 13, 66, 10, 1165, 47, 198, 1008, 39, 5527, 10049, 2884, 2283, 54, 36, 1570, 5, 16, 2, 10050, 195, 245, 1040, 5, 10, 2, 1040, 114, 121, 91, 1570], [14, 2, 4, 223, 1426, 9, 178, 37, 25, 114], [10, 2, 25, 3987, 5, 9, 97, 54, 1254, 10, 2, 32, 4, 230], [1166, 34, 639, 96, 291], [10, 2, 4, 10051, 1167, 10]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TyYMOtpdvYc",
        "outputId": "a329ae39-5dd0-4ce4-8510-69aeefc0331d"
      },
      "source": [
        "ll=len(encoded_text[0])\n",
        "for tw in encoded_text:\n",
        "  l=len(tw)\n",
        "  if l>ll:\n",
        "    ll=l\n",
        "ll"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "62"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hQLwOKVdMAf",
        "outputId": "338c5891-3716-431a-9450-54d73eacd6eb"
      },
      "source": [
        "max_length = 65\n",
        "X = pad_sequences(encoded_text, maxlen=max_length, padding='post')\n",
        "print(X)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1370   44  694 ...    0    0    0]\n",
            " [3979  147    1 ...    0    0    0]\n",
            " [  60   70  107 ...    0    0    0]\n",
            " ...\n",
            " [  41   95  184 ...    0    0    0]\n",
            " [  28  888   64 ...    0    0    0]\n",
            " [9691    6   89 ...    0    0    0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvtoeqHTdMF5",
        "outputId": "b9228b1f-f725-4bd3-d896-dc1ea68f7eb4"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14100, 65)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiKGYgHyfwej"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, test_size = 0.2, stratify = y)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "059Elzdkfwkg"
      },
      "source": [
        "vec_size = 350\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, vec_size, input_length=max_length))\n",
        "\n",
        "model.add(Conv1D(64, 8, activation = 'relu'))\n",
        "model.add(MaxPooling1D(2))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(GlobalMaxPooling1D())\n",
        "\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-k_-mQRyfwo9"
      },
      "source": [
        "model.compile(optimizer='adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdvlqqS_fwsy",
        "outputId": "6c512a77-e5ed-4beb-c2ce-10664ebf4c8b"
      },
      "source": [
        "%%time\n",
        "model.fit(X_train, y_train, epochs = 5, validation_data = (X_test, y_test))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "353/353 [==============================] - 64s 86ms/step - loss: 0.6410 - accuracy: 0.6533 - val_loss: 0.6193 - val_accuracy: 0.6961\n",
            "Epoch 2/5\n",
            "353/353 [==============================] - 29s 83ms/step - loss: 0.5196 - accuracy: 0.7628 - val_loss: 0.5563 - val_accuracy: 0.7479\n",
            "Epoch 3/5\n",
            "353/353 [==============================] - 29s 83ms/step - loss: 0.3547 - accuracy: 0.8609 - val_loss: 0.5466 - val_accuracy: 0.7326\n",
            "Epoch 4/5\n",
            "353/353 [==============================] - 29s 83ms/step - loss: 0.2181 - accuracy: 0.9229 - val_loss: 0.6535 - val_accuracy: 0.7060\n",
            "Epoch 5/5\n",
            "353/353 [==============================] - 29s 83ms/step - loss: 0.1192 - accuracy: 0.9597 - val_loss: 0.7337 - val_accuracy: 0.7138\n",
            "CPU times: user 4min 14s, sys: 8.05 s, total: 4min 22s\n",
            "Wall time: 3min 1s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f2df80ab4d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYc-KEDTi2B0"
      },
      "source": [
        "y_pred=(model.predict(X_test)>0.5)*1"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWyRQH-LuRbt",
        "outputId": "ff6c8082-c97c-47d6-805b-aed310097ed5"
      },
      "source": [
        "y_pred[:5]"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-jY_ta4tsUq",
        "outputId": "f766998f-6588-4e6c-e24a-c4b575940e78"
      },
      "source": [
        "y_true=np.array(y_test)\n",
        "metrics.f1_score(y_true, y_pred, average='macro')\n"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6850095732873103"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbShJHf8w5pU"
      },
      "source": [
        "# Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydc3cvhuuFyz"
      },
      "source": [
        "import os, string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import snowballstemmer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split, cross_val_score \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics\n",
        "from sklearn.base import BaseEstimator, TransformerMixin"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCHACSPaw-gc"
      },
      "source": [
        "le = LabelEncoder()\n",
        "df['target'] = le.fit_transform(df['CATEGORY'])"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxYMqxu5xq8j"
      },
      "source": [
        "import re\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class TextCleaner(BaseEstimator, TransformerMixin):    \n",
        "    def remove_mentions(self, text):        \n",
        "        return re.sub(r'@\\w+', '', text)\n",
        "    \n",
        "    def remove_urls(self, text):        \n",
        "        return re.sub(r'http.?://[^\\s]+[\\s]?', '', text)\n",
        "    \n",
        "    def only_characters(self, text):\n",
        "        return re.sub('[^a-zA-Z\\s]', '', text)\n",
        "    \n",
        "    def remove_extra_spaces(self, text):\n",
        "        text = re.sub(\"\\s+\", ' ', text)\n",
        "        text = text.lstrip()\n",
        "        return text.rstrip()\n",
        "    \n",
        "    def to_lower(self, text):\n",
        "        return text.lower()\n",
        "    \n",
        "    def fix_words(self, text):\n",
        "        text = re.sub(r'\\bthx\\b', 'thanks', text)\n",
        "        text = re.sub(r'\\bu\\b', 'you', text)\n",
        "        text = re.sub(r'\\bhrs\\b', 'hours', text)\n",
        "        text = re.sub(r'\\baa\\b', 'a', text)\n",
        "        text = re.sub(r'\\bflightr\\b', 'flight', text)\n",
        "        text = re.sub(r'\\bur\\b', 'your', text)\n",
        "        text = re.sub(r'\\bhr\\b', 'hour', text)\n",
        "        text = re.sub(r'\\bthru\\b', 'through', text)\n",
        "        text = re.sub(r'\\br\\b', 'are', text)\n",
        "        text = re.sub(r'\\bppl\\b', 'people', text)\n",
        "        text = re.sub(r'\\btix\\b', 'fix', text)\n",
        "        text = re.sub(r'\\bplz\\b', 'please', text)\n",
        "        text = re.sub(r'\\bflightd\\b', 'flighted', text)\n",
        "        text = re.sub(r'\\btmrw\\b', 'tomorrow', text)\n",
        "        text = re.sub(r'\\bthx\\b', 'thanks', text)\n",
        "        text = re.sub(r'\\bpls\\b', 'please', text)\n",
        "        text = re.sub(r'\\bfyi\\b', 'for your information', text)\n",
        "        \n",
        "        text = re.sub(r'\\bheyyyy\\b', 'hey', text)\n",
        "        text = re.sub(r'\\bguyyyys\\b', 'guys', text)\n",
        "        text = re.sub(r'\\byall\\b', 'you all', text)\n",
        "        text = re.sub(r'\\basap\\b', 'as soon as possible', text)\n",
        "        text = re.sub(r'\\bbtw\\b', 'by the way', text)\n",
        "        text = re.sub(r'\\bdm\\b', 'direct message', text)\n",
        "        text = re.sub(r'\\bcudtomers\\b', 'customers', text)\n",
        "        text = re.sub(r'\\bwtf\\b', 'what the fuck', text)\n",
        "        text = re.sub(r'\\biphone\\b', 'phone', text)\n",
        "        text = re.sub(r'\\bmins\\b', 'minutes', text)\n",
        "        text = re.sub(r'\\btv\\b', 'television', text)\n",
        "        text = re.sub(r'\\bokay\\b', 'ok', text)\n",
        "        text = re.sub(r'\\bfeb\\b', 'february', text)\n",
        "        text = re.sub(r'\\byr\\b', 'year', text)\n",
        "        text = re.sub(r'\\bshes\\b', 'she is', text)\n",
        "        text = re.sub(r'\\bnope\\b', 'no', text)\n",
        "        text = re.sub(r'\\bhes\\b', 'he is', text)\n",
        "        text = re.sub(r'\\btill\\b', 'until', text)\n",
        "        text = re.sub(r'\\bomg\\b', 'oh my god', text)\n",
        "        text = re.sub(r'\\btho\\b', 'though', text)\n",
        "        text = re.sub(r'\\bnothappy\\b', 'not happy', text)\n",
        "        return re.sub(r'\\bthankyou\\b', 'thank you', text)\n",
        "        \n",
        "    def fit(self, X, y=None, **fit_params):\n",
        "        return self\n",
        "    \n",
        "    def transform(self, X, **transform_params):        \n",
        "        clean_X = X.apply(self.remove_mentions).apply(self.remove_urls).apply(self.only_characters).apply(self.remove_extra_spaces).apply(self.to_lower).apply(self.fix_words)\n",
        "        return clean_X"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFGl_2RGxUMz"
      },
      "source": [
        "ct = TextCleaner()\n",
        "df['clean_text'] = ct.transform(df['TWEET'])"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S79yjSLexvSk"
      },
      "source": [
        "re_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n",
        "stemmer = snowballstemmer.EnglishStemmer()\n",
        "\n",
        "def tokenize(s): \n",
        "    tokens = re_tok.sub(r' \\1 ', s).split()\n",
        "    return stemmer.stemWords(tokens)"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmXdMYv1x0ns"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df['clean_text'].values, df['target'].values, test_size=0.25, random_state=0)"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOQ5KVVjx3FU"
      },
      "source": [
        "vect = TfidfVectorizer(strip_accents='unicode', tokenizer=tokenize, ngram_range=(1, 2), max_df=0.75, min_df=3, sublinear_tf=True)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJK0V2ASx5XT"
      },
      "source": [
        "tfidf_train = vect.fit_transform(X_train)\n",
        "tfidf_test = vect.transform(X_test)"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FY-ajno10P-W",
        "outputId": "480d58cc-c673-4f89-ff06-99fd43f9eb7b"
      },
      "source": [
        "z=np.unique(y_train)\n",
        "z\n",
        "\n",
        "lk=z[z!=0]\n",
        "lk\n",
        "tfidf_train[y_train==0].sum(0)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "matrix([[164.4841382 ,   0.96943109,   0.62992181, ...,   0.        ,\n",
              "           1.21343101,   1.92864503]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmoEtrMCx7S0"
      },
      "source": [
        "def naive_bayes(x, y):\n",
        "    r = []; b = []\n",
        "    labels = np.unique(y)\n",
        "\n",
        "    for l in labels:\n",
        "        other_l = labels[labels != l]\n",
        "        p = x[y == l].sum(0) + 1\n",
        "        # q = x[(y == other_l[0]) | (y == other_l[1])].sum(0) + 1\n",
        "        q = x[(y == other_l[0])].sum(0) + 1\n",
        "        r.append(np.log((p/p.sum())/(q/q.sum())))\n",
        "        b.append(np.log(len(p)/len(q)))\n",
        "    \n",
        "    return r, b"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W51TMhANyD_z"
      },
      "source": [
        "r, b = naive_bayes(tfidf_train, y_train)\n",
        "\n",
        "pre_preds = []\n",
        "for j in range(len(r)):\n",
        "    pre_preds.append(np.asarray(tfidf_test @ r[j].T + b[j]).reshape(-1))\n",
        "arr = np.array(pre_preds)"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKfbcM881x-8",
        "outputId": "d22b6e63-bbd0-4b32-fc27-9f19cd7a0943"
      },
      "source": [
        "np.argmax(arr.T, 1)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 0, ..., 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pcjDub710hs",
        "outputId": "5387ed53-835f-4861-8c9a-5b376b0d7506"
      },
      "source": [
        "y_test"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 0, ..., 1, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohQ9OlbEyGZL",
        "outputId": "d95bf46c-4189-468c-e4f4-80ad34f49555"
      },
      "source": [
        "metrics.accuracy_score(y_test, np.argmax(arr.T, 1))"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7353191489361702"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "666IFxFU1faM",
        "outputId": "2bafba07-edd6-41c2-cf98-e0a397cb5ced"
      },
      "source": [
        "metrics.f1_score(y_test, np.argmax(arr.T, 1), average='macro')"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6702360078726444"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pLt4ynV2Buo"
      },
      "source": [
        "# Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8dAcoMn2BM7",
        "outputId": "2f063bed-723d-497d-a173-3e384a3ac46d"
      },
      "source": [
        "scores = cross_val_score(LogisticRegression(C=2, dual=True), tfidf_train, y_train, cv=5)"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Solver lbfgs supports only dual=False, got dual=True\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YcMDQgz81iir",
        "outputId": "ef84178a-b78f-4e80-c0ba-b67341a8e216"
      },
      "source": [
        "scores"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([nan, nan, nan, nan, nan])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIOtm-7x2LeU",
        "outputId": "a416debd-6317-4c25-db6b-9dc2e84b136b"
      },
      "source": [
        "\n",
        "m = LogisticRegression()\n",
        "m.fit(tfidf_train, y_train)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnsRevnh2VYz",
        "outputId": "2aaee601-0685-4a67-ec8b-111237201c95"
      },
      "source": [
        "preds = m.predict(tfidf_test)\n",
        "(preds==y_test).mean()"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7577304964539007"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZwy6S3Z2rR0",
        "outputId": "900f87b8-4090-4ccb-9a00-4b6de648ee3e"
      },
      "source": [
        "metrics.f1_score(y_test, preds, average='macro')"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6629877265668243"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqAOIx7F2xoM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}