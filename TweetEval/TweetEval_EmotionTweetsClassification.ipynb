{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TweetEval_EmotionTweetsClassification.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prawizard/TweetsClassification_NLP/blob/main/TweetEval/TweetEval_EmotionTweetsClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnKTSNSw9e4s"
      },
      "source": [
        "# Instructions to Execute:\n",
        "\n",
        "    *   To execute by loading my trained model and evaluating on test data:\n",
        "\n",
        "\n",
        "1.   \n",
        "\n",
        "    #### Go to the following drive link where you can access the weights from my goole drive.\n",
        "\n",
        "    https://drive.google.com/drive/folders/1NyLnCuDTWg476bTpihzhECd54EQjNMma?usp=sharing\n",
        "\n",
        "    #### Download the Folder 'Offensive_ModelWeights'. If it is downloaded as a zip file, please extract the folder. \n",
        "\n",
        "    #### Upload the extracted folder 'Offensive_ModelWeights' to your Google Drive. You will mount your drive in colab runtime and access the .h5 file from there. This way is faster that loading the .h5 file directly to colab due to the large size (~1.1 GB).\n",
        "\n",
        "    #### The trained weights couldn't be uploaded on GitHub as well due to the large size, hence, we will access from Google Drive.\n",
        "    \n",
        "    #### Further instructions on how to do this are provided in Section 14.\n",
        "\n",
        "2. #### Run all cells in sections from Section No. **1 to 7 ONLY**.\n",
        "\n",
        "3. #### Skip the cells afterwards until **Section 14**.\n",
        "\n",
        "4. #### **Execute the cells in Section 14**. Last cell in this section provides the **F1-Score and Accuracy obtained by the model on test set**.\n",
        "\n",
        "\n",
        "\n",
        "    *   To execute by training the model from scratch and then evaluating on test data.\n",
        "\n",
        "\n",
        "\n",
        "#### Execute the Sections from Section No 1 to 12. Section 13 can be ignored, as it is just saving the model.\n",
        "\n",
        "#### Section 14 is to mount your drive on colab and to load the weights(You will have to upload the weights folder from my drive link to your drive before running Section 14 as described in the previous instruction.). Hence, if you're not using my trained model, this section can be ignored too.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOdsQdL_0NGK"
      },
      "source": [
        "# 1 : Install/import the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwM_gT2K7j7w",
        "outputId": "c99c0fd6-6198-44e3-cbaf-835c61e8e31e"
      },
      "source": [
        "!pip install ktrain"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ktrain in /usr/local/lib/python3.7/dist-packages (0.26.2)\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from ktrain) (3.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from ktrain) (2.23.0)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.7/dist-packages (from ktrain) (1.0.8)\n",
            "Requirement already satisfied: seqeval==0.0.19 in /usr/local/lib/python3.7/dist-packages (from ktrain) (0.0.19)\n",
            "Requirement already satisfied: whoosh in /usr/local/lib/python3.7/dist-packages (from ktrain) (2.7.4)\n",
            "Requirement already satisfied: transformers<=4.3.3,>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ktrain) (4.3.3)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from ktrain) (5.5.0)\n",
            "Requirement already satisfied: networkx>=2.3 in /usr/local/lib/python3.7/dist-packages (from ktrain) (2.5.1)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from ktrain) (1.1.5)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.7/dist-packages (from ktrain) (0.42.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from ktrain) (20.9)\n",
            "Requirement already satisfied: fastprogress>=0.1.21 in /usr/local/lib/python3.7/dist-packages (from ktrain) (1.0.0)\n",
            "Requirement already satisfied: scikit-learn==0.23.2 in /usr/local/lib/python3.7/dist-packages (from ktrain) (0.23.2)\n",
            "Requirement already satisfied: keras-bert>=0.86.0 in /usr/local/lib/python3.7/dist-packages (from ktrain) (0.86.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from ktrain) (0.1.95)\n",
            "Requirement already satisfied: cchardet in /usr/local/lib/python3.7/dist-packages (from ktrain) (2.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from ktrain) (1.0.1)\n",
            "Requirement already satisfied: syntok in /usr/local/lib/python3.7/dist-packages (from ktrain) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->ktrain) (2.8.1)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->ktrain) (1.19.5)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->ktrain) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->ktrain) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->ktrain) (1.3.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->ktrain) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->ktrain) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->ktrain) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->ktrain) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from langdetect->ktrain) (1.15.0)\n",
            "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.7/dist-packages (from seqeval==0.0.19->ktrain) (2.4.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<=4.3.3,>=4.0.0->ktrain) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<=4.3.3,>=4.0.0->ktrain) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers<=4.3.3,>=4.0.0->ktrain) (3.10.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers<=4.3.3,>=4.0.0->ktrain) (4.41.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<=4.3.3,>=4.0.0->ktrain) (0.0.44)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers<=4.3.3,>=4.0.0->ktrain) (0.10.2)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->ktrain) (1.0.18)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->ktrain) (5.0.5)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->ktrain) (2.6.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->ktrain) (4.4.2)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython->ktrain) (4.8.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->ktrain) (0.8.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->ktrain) (54.2.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->ktrain) (0.7.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.1->ktrain) (2018.9)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.23.2->ktrain) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.23.2->ktrain) (2.1.0)\n",
            "Requirement already satisfied: keras-transformer>=0.38.0 in /usr/local/lib/python3.7/dist-packages (from keras-bert>=0.86.0->ktrain) (0.38.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from Keras>=2.2.4->seqeval==0.0.19->ktrain) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from Keras>=2.2.4->seqeval==0.0.19->ktrain) (2.10.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers<=4.3.3,>=4.0.0->ktrain) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers<=4.3.3,>=4.0.0->ktrain) (3.7.4.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<=4.3.3,>=4.0.0->ktrain) (7.1.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->ktrain) (0.2.5)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.2->ipython->ktrain) (0.2.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->ktrain) (0.7.0)\n",
            "Requirement already satisfied: keras-multi-head>=0.27.0 in /usr/local/lib/python3.7/dist-packages (from keras-transformer>=0.38.0->keras-bert>=0.86.0->ktrain) (0.27.0)\n",
            "Requirement already satisfied: keras-position-wise-feed-forward>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from keras-transformer>=0.38.0->keras-bert>=0.86.0->ktrain) (0.6.0)\n",
            "Requirement already satisfied: keras-pos-embd>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from keras-transformer>=0.38.0->keras-bert>=0.86.0->ktrain) (0.11.0)\n",
            "Requirement already satisfied: keras-layer-normalization>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from keras-transformer>=0.38.0->keras-bert>=0.86.0->ktrain) (0.14.0)\n",
            "Requirement already satisfied: keras-embed-sim>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from keras-transformer>=0.38.0->keras-bert>=0.86.0->ktrain) (0.8.0)\n",
            "Requirement already satisfied: keras-self-attention==0.46.0 in /usr/local/lib/python3.7/dist-packages (from keras-multi-head>=0.27.0->keras-transformer>=0.38.0->keras-bert>=0.86.0->ktrain) (0.46.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BDe2xwg7lcN"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ktrain\n",
        "from ktrain import text\n",
        "import tensorflow as tf\n",
        "import re\n",
        "import requests\n",
        "import sklearn.metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUSnw8my0TQa"
      },
      "source": [
        "# 2 : Define Constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIkGsuc478mN"
      },
      "source": [
        "TRAIN_TEXT_URL=\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/emotion/train_text.txt\"\n",
        "TRAIN_LABELS_URL=\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/emotion/train_labels.txt\"\n",
        "VAL_TEXT_URL=\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/emotion/val_text.txt\"\n",
        "VAL_LABELS_URL=\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/emotion/val_labels.txt\"\n",
        "TEST_TEXT_URL=\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/emotion/test_text.txt\"\n",
        "TEST_LABELS_URL=\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/emotion/test_labels.txt\"\n",
        "VOCAB_SIZE=2000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIUiL2Fx0VkA"
      },
      "source": [
        "# 3 : Fetch the Data using URLs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZrCV4xE787w",
        "outputId": "c08b2688-8327-4695-f4e1-8dc19644ba98"
      },
      "source": [
        "r = requests.get(TRAIN_TEXT_URL, allow_redirects=True)\n",
        "open('train_text.txt', 'wb').write(r.content)\n",
        "\n",
        "r = requests.get(TRAIN_LABELS_URL, allow_redirects=True)\n",
        "open('train_labels.txt', 'wb').write(r.content)\n",
        "\n",
        "r = requests.get(VAL_TEXT_URL, allow_redirects=True)\n",
        "open('val_text.txt', 'wb').write(r.content)\n",
        "\n",
        "r = requests.get(VAL_LABELS_URL, allow_redirects=True)\n",
        "open('val_labels.txt', 'wb').write(r.content)\n",
        "\n",
        "r = requests.get(TEST_TEXT_URL, allow_redirects=True)\n",
        "open('test_text.txt', 'wb').write(r.content)\n",
        "\n",
        "r = requests.get(TEST_LABELS_URL, allow_redirects=True)\n",
        "open('test_labels.txt', 'wb').write(r.content)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2842"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bror6ixP0dFy"
      },
      "source": [
        "# 4 : Access the data from files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3pRAqQi79Ay"
      },
      "source": [
        "stream=open(\"train_text.txt\")\n",
        "tweets=stream.readlines()\n",
        "stream.close()\n",
        "val_stream=open(\"val_text.txt\")\n",
        "val_tweets=val_stream.readlines()\n",
        "val_stream.close()\n",
        "test_stream=open(\"test_text.txt\")\n",
        "test_tweets=test_stream.readlines()\n",
        "test_stream.close()\n",
        "\n",
        "# Labels\n",
        "stream=open(\"train_labels.txt\")\n",
        "tweetsLabels=stream.readlines()\n",
        "stream.close()\n",
        "val_stream=open(\"val_labels.txt\")\n",
        "val_tweetsLabels=val_stream.readlines()\n",
        "val_stream.close()\n",
        "test_stream=open(\"test_labels.txt\")\n",
        "test_tweetsLabels=test_stream.readlines()\n",
        "test_stream.close()\n",
        "\n",
        "\n",
        "# Labels\n",
        "labels=[0]*len(tweetsLabels)\n",
        "for i in range(len(tweetsLabels)):\n",
        "  if tweetsLabels[i].find('\\n')!=-1:\n",
        "    labels[i]=int(re.sub('\\n', '', tweetsLabels[i]))\n",
        "val_labels=[0]*len(val_tweetsLabels)\n",
        "for i in range(len(val_tweetsLabels)):\n",
        "  if val_tweetsLabels[i].find('\\n')!=-1:\n",
        "    val_labels[i]=int(re.sub('\\n', '', val_tweetsLabels[i]))\n",
        "test_labels=[0]*len(test_tweetsLabels)\n",
        "for i in range(len(test_tweetsLabels)):\n",
        "  if test_tweetsLabels[i].find('\\n')!=-1:\n",
        "    test_labels[i]=int(re.sub('\\n', '', test_tweetsLabels[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duWTcaNO79ES",
        "outputId": "63286539-b0a0-42a0-c2cc-ee80895cb515"
      },
      "source": [
        "print('Samples in Training set : ',len(labels),', Validation set : ', len(val_labels),', Test set : ', len(test_labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Samples in Training set :  3257 , Validation set :  374 , Test set :  1421\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQYdsf4g0i3I"
      },
      "source": [
        "# 5 : Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVJDkn340kSQ"
      },
      "source": [
        "## 5.1 : Remove the twitter handles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSR8FYQ18PuB"
      },
      "source": [
        "for i in range(len(tweets)):\n",
        "  if tweets[i].find('@user')!=-1:\n",
        "    tweets[i]=re.sub('@user', '', tweets[i])\n",
        "    tweets[i]=re.sub('#+', '', tweets[i])\n",
        "\n",
        "for i in range(len(val_tweets)):\n",
        "  if val_tweets[i].find('@user')!=-1:\n",
        "    val_tweets[i]=re.sub('@user', '', val_tweets[i])\n",
        "    val_tweets[i]=re.sub('#+', '', val_tweets[i])\n",
        "\n",
        "for i in range(len(test_tweets)):\n",
        "  if test_tweets[i].find('@user')!=-1:\n",
        "    test_tweets[i]=re.sub('@user', '', test_tweets[i])\n",
        "    test_tweets[i]=re.sub('#+', '', test_tweets[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7wQGfwf0oXh"
      },
      "source": [
        "## 5.2 : Remove the unnecessary hashtags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwTTLvOC8TtC"
      },
      "source": [
        "for i in range(len(tweets)):\n",
        "  if tweets[i].find('#[a-zA-Z]+')!=-1:\n",
        "    tweets[i]=re.sub('#[a-zA-Z]+', '', tweets[i])\n",
        "\n",
        "for i in range(len(val_tweets)):\n",
        "  if val_tweets[i].find('#[a-zA-Z]+')!=-1:\n",
        "    val_tweets[i]=re.sub('#[a-zA-Z]+', '', val_tweets[i])\n",
        "\n",
        "for i in range(len(test_tweets)):\n",
        "  if test_tweets[i].find('#[a-zA-Z]+')!=-1:\n",
        "    test_tweets[i]=re.sub('#[a-zA-Z]+', '', test_tweets[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5o2PvW40qwI"
      },
      "source": [
        "## 5.3 : Remove characters like \\\\n and unnecessary dots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a7i68Eg86zM"
      },
      "source": [
        "for i in range(len(tweets)):\n",
        "  if tweets[i].find('\\n')!=-1:\n",
        "    tweets[i]=re.sub('\\n', '', tweets[i])\n",
        "\n",
        "for i in range(len(val_tweets)):\n",
        "  if val_tweets[i].find('\\n')!=-1:\n",
        "    val_tweets[i]=re.sub('\\n', '', val_tweets[i])\n",
        "\n",
        "for i in range(len(test_tweets)):\n",
        "  if test_tweets[i].find('\\n')!=-1:\n",
        "    test_tweets[i]=re.sub('\\n', '', test_tweets[i])\n",
        "\n",
        "# Unnecessary dots\n",
        "p='\\.\\.\\.|\\.\\.'\n",
        "\n",
        "for i in range(len(tweets)):\n",
        "  tweets[i]=re.sub(p, '', tweets[i])\n",
        "  tweets[i]=tweets[i].lower()\n",
        "\n",
        "for i in range(len(val_tweets)):\n",
        "  val_tweets[i]=re.sub(p, '', val_tweets[i])\n",
        "  val_tweets[i]=val_tweets[i].lower()\n",
        "\n",
        "for i in range(len(test_tweets)):\n",
        "  test_tweets[i]=re.sub(p, '', test_tweets[i])\n",
        "  test_tweets[i]=test_tweets[i].lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIqgtu6Z01oY"
      },
      "source": [
        "# 6 : Store the Data Processed In a Data Frame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGECp9og-CMc"
      },
      "source": [
        "rows=[]\n",
        "rowIndices=[]\n",
        "for i in range(len(tweets)):\n",
        "  rows.append({\"TWEET\":tweets[i], \"CATEGORY\":labels[i]})\n",
        "  rowIndices.append(i+1)\n",
        "train_df=pd.DataFrame(rows, index=rowIndices)\n",
        "\n",
        "val_rows=[]\n",
        "val_rowIndices=[]\n",
        "for i in range(len(val_tweets)):\n",
        "  val_rows.append({\"TWEET\":val_tweets[i], \"CATEGORY\":val_labels[i]})\n",
        "  val_rowIndices.append(i+1)\n",
        "val_df=pd.DataFrame(val_rows, index=val_rowIndices)\n",
        "\n",
        "test_rows=[]\n",
        "test_rowIndices=[]\n",
        "for i in range(len(test_tweets)):\n",
        "  test_rows.append({\"TWEET\":test_tweets[i], \"CATEGORY\":test_labels[i]})\n",
        "  test_rowIndices.append(i+1)\n",
        "test_df=pd.DataFrame(test_rows, index=test_rowIndices)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRUkdRNW06SY"
      },
      "source": [
        "# 7 : Segregate the Data Into Training and Validation/Dev Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "S2A_9OdM-E90",
        "outputId": "b7ba9088-2afd-4230-d5c4-759046104dbb"
      },
      "source": [
        "(X_train, y_train), (X_test, y_test), preproc = text.texts_from_df(train_df=train_df,\n",
        "                                                                   text_column = 'TWEET',\n",
        "                                                                   label_columns = 'CATEGORY',\n",
        "                                                                   val_df = val_df,\n",
        "                                                                   maxlen = 65,\n",
        "                                                                   preprocess_mode = 'bert')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['CATEGORY_0', 'CATEGORY_1', 'CATEGORY_2', 'CATEGORY_3']\n",
            "   CATEGORY_0  CATEGORY_1  CATEGORY_2  CATEGORY_3\n",
            "1         0.0         0.0         1.0         0.0\n",
            "2         1.0         0.0         0.0         0.0\n",
            "3         0.0         1.0         0.0         0.0\n",
            "4         1.0         0.0         0.0         0.0\n",
            "5         0.0         0.0         0.0         1.0\n",
            "['CATEGORY_0', 'CATEGORY_1', 'CATEGORY_2', 'CATEGORY_3']\n",
            "   CATEGORY_0  CATEGORY_1  CATEGORY_2  CATEGORY_3\n",
            "1         1.0         0.0         0.0         0.0\n",
            "2         1.0         0.0         0.0         0.0\n",
            "3         1.0         0.0         0.0         0.0\n",
            "4         1.0         0.0         0.0         0.0\n",
            "5         1.0         0.0         0.0         0.0\n",
            "preprocessing train...\n",
            "language: en\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "done."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Is Multi-Label? False\n",
            "preprocessing test...\n",
            "language: en\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "done."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Yuhq1jhaLQX"
      },
      "source": [
        "# * For loading my trained weights and evaluating instead of training the model again, skip all the cells below until the Section number 14 \" (Running the Model Using Weights (Pretrained))\" and run all the cells that follow thereafter. *\n",
        "\n",
        "# Instructions on how the trained weights can be loaded are provided in that section. (Number 14)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5zJvH-4aLQZ"
      },
      "source": [
        "# *Otherwise, To Train The Model Again, Continue Executing the Cells Below Until the Section 12 \"(Running the Model Using Weights (Pretrained))\". Section 12 gives the result after training the model again and evaluating on test data. *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdmoVWkkO6in"
      },
      "source": [
        "# 8 : Using BERT for pretrained weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGxOSU32_De0",
        "outputId": "9b2a5944-3c04-4dcc-ecae-4078521af18d"
      },
      "source": [
        "model = text.text_classifier(name = 'bert',\n",
        "                             train_data = (X_train, y_train),\n",
        "                             preproc = preproc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Is Multi-Label? False\n",
            "maxlen is 65\n",
            "done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4yKTA2q1cYo"
      },
      "source": [
        "# 9 : Tuning the hyper parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfQztq9J_GQG"
      },
      "source": [
        "learner = ktrain.get_learner(model=model, train_data=(X_train, y_train),\n",
        "                   val_data = (X_test, y_test),\n",
        "                   batch_size = 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ineU4uT_1fKA"
      },
      "source": [
        "# 10 : Fit the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdewFixF_JQ0",
        "outputId": "3dab7c84-effd-4471-dd5b-4a5d0a71adf2"
      },
      "source": [
        "learner.fit_onecycle(lr = 2e-5, epochs = 3)\n",
        "\n",
        "predictor = ktrain.get_predictor(learner.model, preproc)\n",
        "predictor.save('/content/drive/My Drive/bert')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "begin training using onecycle policy with max lr of 2e-05...\n",
            "Epoch 1/3\n",
            "1086/1086 [==============================] - 358s 313ms/step - loss: 1.1576 - accuracy: 0.4772 - val_loss: 0.5872 - val_accuracy: 0.7995\n",
            "Epoch 2/3\n",
            "1086/1086 [==============================] - 331s 305ms/step - loss: 0.4856 - accuracy: 0.8282 - val_loss: 0.5678 - val_accuracy: 0.7781\n",
            "Epoch 3/3\n",
            "1086/1086 [==============================] - 331s 305ms/step - loss: 0.1661 - accuracy: 0.9480 - val_loss: 0.6696 - val_accuracy: 0.7968\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezdSfU8_1i9x"
      },
      "source": [
        "# 11 : Evaluate the Model Performance on Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTzxXple_MaM"
      },
      "source": [
        "data = test_df['TWEET'].tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8t6XivQA_OZM"
      },
      "source": [
        "bert_pred=predictor.predict(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-Wi1xxLgZFl",
        "outputId": "e5bb77ee-cf83-4422-8364-f6e878e73d13"
      },
      "source": [
        "bert_pred[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['CATEGORY_3',\n",
              " 'CATEGORY_0',\n",
              " 'CATEGORY_3',\n",
              " 'CATEGORY_1',\n",
              " 'CATEGORY_1',\n",
              " 'CATEGORY_0',\n",
              " 'CATEGORY_3',\n",
              " 'CATEGORY_3',\n",
              " 'CATEGORY_1',\n",
              " 'CATEGORY_0']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-UoLLmw_QKw"
      },
      "source": [
        "y_true=np.array(test_df['CATEGORY'].tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-woOHmZgdoI",
        "outputId": "c2251f75-9b7e-4732-a1a6-d9bd98181708"
      },
      "source": [
        "y_true[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3, 0, 3, 1, 1, 0, 3, 3, 3, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niMds208_RsI"
      },
      "source": [
        "res_bert=[]\n",
        "for i in range(len(bert_pred)):\n",
        "  if bert_pred[i]=='CATEGORY_0':\n",
        "    res_bert.append(0)\n",
        "  elif bert_pred[i]=='CATEGORY_1':\n",
        "    res_bert.append(1)\n",
        "  elif bert_pred[i]=='CATEGORY_2':\n",
        "    res_bert.append(2)\n",
        "  else:\n",
        "    res_bert.append(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJQjlWGu1mbY"
      },
      "source": [
        "# 12 : Accuracy and F1-Score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfzaVkvR_Tq1",
        "outputId": "fdc01bcd-c1a5-49ed-a742-595724515639"
      },
      "source": [
        "res_bert=np.array(res_bert)\n",
        "np.mean(res_bert==y_true)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8142153413089374"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1c581Z0_XUE",
        "outputId": "c21b2335-c802-40f5-f3fd-8ef76b3aa940"
      },
      "source": [
        "F1_SCORE=sklearn.metrics.f1_score(y_true, res_bert, average='macro')\n",
        "print('F1-SCORE OBTAINED :',round(F1_SCORE*100, 2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1-SCORE OBTAINED : 78.38\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMSw-wKU1riZ"
      },
      "source": [
        "# 13 : Saving the Trained Weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6DPCDTNM7WF"
      },
      "source": [
        "predictor.save('/content/EmotionTrainedWeights/Weights')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FFGgFn8NMrc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43fa4c3e-2691-4ed9-dd5b-460a2547f67b"
      },
      "source": [
        "!zip -r /content/Emotion_Weights.zip /content/EmotionTrainedWeights/Weights"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: content/EmotionTrainedWeights/Weights/ (stored 0%)\n",
            "  adding: content/EmotionTrainedWeights/Weights/tf_model.preproc (deflated 52%)\n",
            "  adding: content/EmotionTrainedWeights/Weights/tf_model.h5 (deflated 18%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cS6HQqSlNa0I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d0e5a5c5-9dad-44d0-f310-0c8db9e2a2d9"
      },
      "source": [
        "from google.colab import files\n",
        "files.download('/content/Emotion_Weights.zip')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_8160ddc4-d979-4a5a-993e-5d4e987d9fbf\", \"Emotion_Weights.zip\", 1077657476)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuHAeQAj1zU5"
      },
      "source": [
        "# 14 : Running the Model Using Weights (Pretrained)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9N2PJE_Wa94c"
      },
      "source": [
        "### Run the cell below. \n",
        "* You must have uploaded my trained weights folder to your drive, as described at the top of this notebook. Use the same google account here to which you uploaded the folder. *\n",
        "### 1. Click on the link following \"Go to this URL in a browser:\"\n",
        "### 2. Select your google account/ Sign-in to your google account.\n",
        "### 3. Click 'Allow'\n",
        "### 4. Copy the link in that tab and paste it in the box provided. (With the box label 'Enter your authorization code:') Use Ctrl+V to paste as right click to paste might not work.\n",
        "### 5. Hit the Enter Key. Now you can see the folder 'gdrive' with files/folders in your drive ready for access in colab runtime. \n",
        "\n",
        "### We will use these weights folders to load the trained weights and evaluate the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fKPTF6JRKn9",
        "outputId": "e77b41e2-fca3-422b-ae13-678863e4c948"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "!cp '/content/gdrive/My Drive/Emotion_ModelWeights' Emotion_Weights_Drive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "cp: -r not specified; omitting directory '/content/gdrive/My Drive/Emotion_ModelWeights'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9VdMKp415k4"
      },
      "source": [
        "### Load the weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFsvwWDsxQld"
      },
      "source": [
        "predictor_load = ktrain.load_predictor('/content/gdrive/My Drive/Emotion_ModelWeights')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvR4SEBbRj3C"
      },
      "source": [
        "## Evaluate on Test Data Using these loaded weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inQI54XRRgqL"
      },
      "source": [
        "data = test_df['TWEET'].tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31_iag9RRpZC"
      },
      "source": [
        "bert_pred_load=predictor_load.predict(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbfIOSgbRrFi"
      },
      "source": [
        "y_true=np.array(test_df['CATEGORY'].tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oh4Z4DOcRsm0"
      },
      "source": [
        "res_bert_load=[]\n",
        "for i in range(len(bert_pred_load)):\n",
        "  if bert_pred_load[i]=='CATEGORY_0':\n",
        "    res_bert_load.append(0)\n",
        "  elif bert_pred[i]=='CATEGORY_1':\n",
        "    res_bert_load.append(1)\n",
        "  elif bert_pred[i]=='CATEGORY_2':\n",
        "    res_bert_load.append(2)\n",
        "  else:\n",
        "    res_bert_load.append(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLzcr8kmR0SR"
      },
      "source": [
        "## ACCURACY and F1_SCORE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UKCsbAkRygK",
        "outputId": "1a62e5ef-0800-4c57-92f4-8ae6375aac16"
      },
      "source": [
        "res_bert_load=np.array(res_bert_load)\n",
        "print('ACCURACY OBTAINED :', round(np.mean(res_bert_load==y_true)*100,2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ACCURACY OBTAINED : 81.42\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8oXCTkqR3gi",
        "outputId": "24a5ca7d-e6de-454e-b855-1fa7f92a9612"
      },
      "source": [
        "F1_SCORE=sklearn.metrics.f1_score(y_true, res_bert_load, average='macro')\n",
        "print('F1_SCORE OBTAINED :', round(F1_SCORE*100, 2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1_SCORE OBTAINED : 78.38\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHELpuzvzoQ6"
      },
      "source": [
        "# while True:pass"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}