{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TweetsClassification.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMqXE2oAHGBAELQgJgx0fFg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prawizard/TweetsClassification_NLP/blob/main/TweetsClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vaBj9Kovntd"
      },
      "source": [
        ""
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDSOG_yBaYzP"
      },
      "source": [
        "#Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecA14DWGgBd8"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import urllib.request\r\n",
        "import nltk\r\n",
        "from nltk.stem import PorterStemmer\r\n",
        "from nltk.stem import SnowballStemmer\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "from sklearn.metrics import f1_score\r\n",
        "import re\r\n",
        "import requests\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4k7Ey0qzQuZ"
      },
      "source": [
        "# Constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hor7wRZdzUmo"
      },
      "source": [
        "TRAIN_TEXT_URL=\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/offensive/train_text.txt\"\r\n",
        "TRAIN_LABELS_URL=\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/offensive/train_labels.txt\"\r\n",
        "VAL_TEXT_URL=\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/offensive/val_text.txt\"\r\n",
        "VAL_LABELS_URL=\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/offensive/val_labels.txt\"\r\n",
        "TEST_TEXT_URL=\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/offensive/test_text.txt\"\r\n",
        "TEST_LABELS_URL=\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/offensive/test_labels.txt\"\r\n",
        "VOCAB_SIZE=2000\r\n",
        "LAPLACE_CONST=0.3"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJGNQyFragCW"
      },
      "source": [
        "#Access Data from the Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2LDDCbjV0BT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff764ede-b1d4-4d54-c3bd-ae4976b6b1d9"
      },
      "source": [
        "r = requests.get(TRAIN_TEXT_URL, allow_redirects=True)\r\n",
        "open('train_text.txt', 'wb').write(r.content)\r\n",
        "\r\n",
        "r = requests.get(TRAIN_LABELS_URL, allow_redirects=True)\r\n",
        "open('train_labels.txt', 'wb').write(r.content)\r\n",
        "\r\n",
        "r = requests.get(VAL_TEXT_URL, allow_redirects=True)\r\n",
        "open('val_text.txt', 'wb').write(r.content)\r\n",
        "\r\n",
        "r = requests.get(VAL_LABELS_URL, allow_redirects=True)\r\n",
        "open('val_labels.txt', 'wb').write(r.content)\r\n",
        "\r\n",
        "r = requests.get(TEST_TEXT_URL, allow_redirects=True)\r\n",
        "open('test_text.txt', 'wb').write(r.content)\r\n",
        "\r\n",
        "r = requests.get(TEST_LABELS_URL, allow_redirects=True)\r\n",
        "open('test_labels.txt', 'wb').write(r.content)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1720"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDnI5uoYI4yE"
      },
      "source": [
        "# temki = urllib.request.urlopen(TARGET_URL)\r\n",
        "# type(temki)\r\n",
        "# import requests\r\n",
        "\r\n",
        "# response = requests.get(TARGET_URL)\r\n",
        "# data = response.text\r\n",
        "# type(data)\r\n",
        "# data[1]\r\n",
        "stream=open(\"train_text.txt\")\r\n",
        "tweets=stream.readlines()\r\n",
        "stream.close()\r\n",
        "\r\n",
        "val_stream=open(\"val_text.txt\")\r\n",
        "val_tweets=val_stream.readlines()\r\n",
        "val_stream.close()\r\n",
        "\r\n",
        "test_stream=open(\"test_text.txt\")\r\n",
        "test_tweets=test_stream.readlines()\r\n",
        "test_stream.close()\r\n"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQiTlMDjUAiw",
        "outputId": "d494ce07-7524-467b-bc7b-bc548146791d"
      },
      "source": [
        "print(len(tweets), len(val_tweets))"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11916 1324\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SB5yEiTZUIAS"
      },
      "source": [
        "for i in range(len(tweets)):\r\n",
        "  tweets[i]=str(re.sub('\\n', '', tweets[i]))\r\n",
        "  if tweets[i].find('@user')!=-1:\r\n",
        "    # tweets[i] = tweets[i].replace('@user',\"\")\r\n",
        "    tweets[i]=re.sub('@user', '', tweets[i])\r\n",
        "\r\n",
        "for i in range(len(val_tweets)):\r\n",
        "  val_tweets[i]=str(re.sub('\\n', '', val_tweets[i]))\r\n",
        "  if val_tweets[i].find('@user')!=-1:\r\n",
        "    val_tweets[i]=re.sub('@user', '', val_tweets[i])\r\n",
        "\r\n",
        "for i in range(len(test_tweets)):\r\n",
        "  test_tweets[i]=str(re.sub('\\n', '', test_tweets[i]))\r\n",
        "  if test_tweets[i].find('@user')!=-1:\r\n",
        "    test_tweets[i]=re.sub('@user', '', test_tweets[i])\r\n",
        "# tweets    \r\n",
        "# Words like effing converted to VetsResistSquadron"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7khxLmiYQJw"
      },
      "source": [
        "stream=open(\"train_labels.txt\")\r\n",
        "tweetsLabels=stream.readlines()\r\n",
        "stream.close()\r\n",
        "\r\n",
        "val_stream=open(\"val_labels.txt\")\r\n",
        "val_tweetsLabels=val_stream.readlines()\r\n",
        "val_stream.close()\r\n",
        "\r\n",
        "test_stream=open(\"test_labels.txt\")\r\n",
        "test_tweetsLabels=test_stream.readlines()\r\n",
        "test_stream.close()"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQ0Thi--Y15C"
      },
      "source": [
        "labels=[0]*len(tweetsLabels)\r\n",
        "for i in range(len(tweetsLabels)):\r\n",
        "  if tweetsLabels[i].find('\\n')!=-1:\r\n",
        "    # tweets[i] = tweets[i].replace('@user',\"\")\r\n",
        "    labels[i]=int(re.sub('\\n', '', tweetsLabels[i]))\r\n",
        "# labels    \r\n",
        "# Words like effing converted to VetsResistSquadron\r\n",
        "\r\n",
        "val_labels=[0]*len(val_tweetsLabels)\r\n",
        "for i in range(len(val_tweetsLabels)):\r\n",
        "  if val_tweetsLabels[i].find('\\n')!=-1:\r\n",
        "    # tweets[i] = tweets[i].replace('@user',\"\")\r\n",
        "    val_labels[i]=int(re.sub('\\n', '', val_tweetsLabels[i]))\r\n",
        "\r\n",
        "test_labels=[0]*len(test_tweetsLabels)\r\n",
        "for i in range(len(test_tweetsLabels)):\r\n",
        "  if test_tweetsLabels[i].find('\\n')!=-1:\r\n",
        "    # tweets[i] = tweets[i].replace('@user',\"\")\r\n",
        "    test_labels[i]=int(re.sub('\\n', '', test_tweetsLabels[i]))"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "st70a2eswdN2",
        "outputId": "59a8deeb-af4c-477e-fdf5-40adc5b19cdd"
      },
      "source": [
        "tweets[:5]"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[' Bono... who cares. Soon people will understand that they gain nothing from following a phony celebrity. Become a Leader of your people instead or help and support your fellow countrymen. ',\n",
              " ' Eight years the republicans denied obamaâ€™s picks. Breitbarters outrage is as phony as their fake president. ',\n",
              " ' Get him some line help. He is gonna be just fine. As the game went on you could see him progressing more with his reads. He brought what has been missing. The deep ball presence. Now he just needs a little more time ',\n",
              " '  She is great. Hi Fiona! ',\n",
              " \" She has become a parody unto herself? She has certainly taken some heat for being such an....well idiot. Could be optic too  Who know with Liberals  They're all optics.  No substance \"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaIdI52AwYN3"
      },
      "source": [
        "\r\n",
        "\r\n",
        "sentences = [\r\n",
        "    'i love my dog',\r\n",
        "    'I, love my cat',\r\n",
        "    'You love my dog!'\r\n",
        "]\r\n",
        "\r\n",
        "tokenizer = Tokenizer(num_words = 100, oov_token=\"<OOV>\")\r\n",
        "tokenizer.fit_on_texts(tweets)\r\n",
        "word_index = tokenizer.word_index\r\n",
        "# print(word_index)"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5pmgciQxscX"
      },
      "source": [
        "sequences = tokenizer.texts_to_sequences(tweets)\r\n",
        "test_sequences = tokenizer.texts_to_sequences(test_tweets)\r\n",
        "# padded = pad_sequences(sequences, maxlen=5)\r\n",
        "\r\n",
        "# padded = pad_sequences(sequences, padding='post')\r\n",
        "padded = pad_sequences(sequences)\r\n",
        "test_padded = pad_sequences(test_sequences)\r\n",
        "# print(\"\\nWord Index = \" , word_index)\r\n",
        "# print(\"\\nSequences = \" , sequences)\r\n",
        "# print(\"\\nPadded Sequences:\")\r\n",
        "# print(padded)\r\n",
        "\r\n",
        "\r\n",
        "# Try with words that the tokenizer wasn't fit to\r\n",
        "# test_data = [\r\n",
        "#     'i really love my dog',\r\n",
        "#     'my dog loves my manatee'\r\n",
        "# ]\r\n",
        "\r\n",
        "# test_seq = tokenizer.texts_to_sequences(test_data)\r\n",
        "# print(\"\\nTest Sequence = \", test_seq)\r\n",
        "\r\n",
        "# padded = pad_sequences(test_seq, maxlen=10)\r\n",
        "# print(\"\\nPadded Test Sequence: \")\r\n",
        "# print(padded)"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CumNigkzzd_M",
        "outputId": "40e39ae9-18cd-428e-9ecd-6fbfdd3b4ab7"
      },
      "source": [
        "padded[0:5]"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         1, 44,  1,  1, 46, 43,  1, 13, 20,  1,  1, 49,  1,  5,  1,  1,\n",
              "         1,  5,  1,  8, 30, 46,  1, 51,  1,  7,  1, 30,  1,  1],\n",
              "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  1,\n",
              "         2,  1,  1,  1,  1,  1,  1,  3, 34,  1, 34, 58,  1,  1],\n",
              "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0, 62, 71,  1,  1,  1, 11,  3,  1, 23, 39,  1, 34,  2,\n",
              "         1,  1, 18,  6,  1, 93, 71,  1, 68, 21, 48,  1, 11,  1, 32, 55,\n",
              "        92,  1,  2,  1,  1,  1, 73, 11, 39,  1,  5,  1, 68, 84],\n",
              "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0, 15,  3,  1,  1,  1],\n",
              "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 15, 55,\n",
              "         1,  5,  1,  1,  1, 15, 55,  1,  1,  1,  1, 14, 94,  1, 56,  1,\n",
              "         1,  1, 23,  1,  1, 44, 64, 21, 24,  1, 27,  1, 47,  1]],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqwgFC54z01o",
        "outputId": "ce043ce3-f04e-49ae-e31b-fa6688bce67a"
      },
      "source": [
        "padded.shape"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11916, 62)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xhhwb4jO4LZ0",
        "outputId": "f100c3a4-892e-4027-96b0-54fc85d742ff"
      },
      "source": [
        "test_padded.shape"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(860, 57)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfdsCrnG55lZ"
      },
      "source": [
        "vocab_size = 10000\r\n",
        "embedding_dim = 16\r\n",
        "max_length = 100\r\n",
        "trunc_type='post'\r\n",
        "padding_type='post'\r\n",
        "oov_tok = \"<OOV>\"\r\n",
        "training_size = 20000"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H76IK4lE66bx"
      },
      "source": [
        "training_padded = np.array(padded)\r\n",
        "training_labels = np.array(labels)\r\n",
        "testing_padded = np.array(test_padded)\r\n",
        "testing_labels = np.array(test_labels)"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5mlQVze5EXT"
      },
      "source": [
        "model = tf.keras.Sequential([\r\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\r\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),\r\n",
        "    tf.keras.layers.Dense(24, activation='relu'),\r\n",
        "    tf.keras.layers.Dense(16, activation='relu'),\r\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\r\n",
        "])\r\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9Xnk1Ho6A2q",
        "outputId": "a4604f6c-3739-421e-b083-5ca21f5e1f82"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_5 (Embedding)      (None, 100, 16)           160000    \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_5 ( (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 24)                408       \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 16)                400       \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 160,825\n",
            "Trainable params: 160,825\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtU-YNak6Pzr"
      },
      "source": [
        "num_epochs = 50\r\n",
        "history = model.fit(training_padded, training_labels, epochs=num_epochs, validation_data=(testing_padded, testing_labels), verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-hgB-u96cR6"
      },
      "source": [
        ""
      ],
      "execution_count": 92,
      "outputs": []
    }
  ]
}